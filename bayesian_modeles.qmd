---
title: "Introduction to bayesian modeles for ecology"
author : Coline - Jules - Mathis - Romane - Laura
bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

```{r setup}
#| include: false

library(tidyverse)
library(ggforce)
library(deSolve)
library(rjags)
library(ggmcmc)



```

# Introduction

-   History of Bayesian concept.
-   Philosophy behind it. Frequentist approach : "What is the probability to observe my data given my model."

Bayesian approach : "What is the probability of m y model given my data."

# General theory behind Bayesian approach.

Explanation in the special case of conjugation between prior and posterior

## Main objective, for context

First, let's take a simple example to explain the idea behind the Bayesian method. We want to estimate the mean abundance per $m^2$ of one fungus in a forest. To do that, we set up some sampling areas in which we count the number of mushrooms. We can simulate the data by taking random observations in a Poisson distribution. Let's suppose that our study has 200 sampling area. **Because it is count data, the number of mushrooms that we will count should follow a Poisson distribution :**

```{r}
n_sample = 200 # sampling area
lbda = 5 # mean of poisson distribution
pois_distr = dpois(1:20, lambda = lbda)
plot(pois_distr,type ="h",
     lwd = 2, col = 'blue',
     xlab = "Mushroom Count", 
     ylab = expression(paste( 'Density or ','[y]')) )

```

To simulate the sampling campaign we are taking values in this Poisson distribution. In our case, those will be used to estimate the mean number of mushrooms in the forest. ($\lambda = 5$ is already known because we simulate the data, but in reality, this is an unknown).

```{r}
set.seed(1000)

Y = rpois(n = n_sample, lambda = lbda)

hist(Y)
```

**So let's pretend that we don't know the** $\lambda$. We want to find the the value of the mean number of mushrooms we will call $\hat{\lambda}$ and we also want to know the probability of this $\hat{\lambda}$. The Bayesian method will give us a range of estimated mean $\hat{\lambda}$ and the probability of those values to be true knowing the observation $Y$. This is what we call the posterior distribution. We can simply write this as follows $[\hat{\lambda} \mid Y]$ which is the probability of $\hat{\lambda}$ knowing our observations. This probability can be found with the equation :

$$
[\hat{\lambda} \mid Y] = [Y \mid \hat{\lambda}  ]\cdot[\hat{\lambda}]
$$

The two components of the right-hand side of the equation are $[Y \mid \hat{\lambda} ]$ the likelihood of our data and $[\hat{\lambda}]$ the prior distribution. First, let's start with the likelihood. Our data follow a Poisson distribution so our likelihood will follow a Poisson distribution :

$$
L(\hat{\lambda} ; y)=[y \mid \hat{\lambda}] = \frac{e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y}}{y!}
$$

This is a first good step, but there is a small issue here, this formula isn't completely usable in this form. This is because it can only take one observation. In English words, it is like asking what is the probability of one observation (one count of mushroom) given a model with a mean $\hat{\lambda}$. This form isn't powerful enough because it uses only one observation. What we want is to use all the data that we have, we want to know the probability of all the observations given a model with a mean $\hat{\lambda}$. To do so we can write the likelihood of all our data $Y$ as the product of the likelihood of each observation $y_i$. (We are allowed to do this only because observations are independent)

$$
\begin{align}
[Y \mid \hat{\lambda}] &= \prod^{n}_{i=1}[y_i \mid \hat{\lambda}] \\
&=\prod^{n}_{i=1}\frac{e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y_i}}{y_i!} \\
&\propto \prod^{n}_{i=1}e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y_i}\\
\end{align}
$$

As you can see we don't keep the $\frac{1}{y_i!}$, it is because we are only interested in the terms that are impacted by $\hat{\lambda}$. The last form which is proportional to the likelihood function has a more convenient form for the next step. Let's rearrange the function in a more convenient form

$$
\begin{align}
\prod^{n}_{i=1}e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y_i}&= e^{-n \cdot\hat{\lambda}} \cdot\hat{\lambda}^{\sum^{n}_{i=1} y_i}\\
&= e^{-n \cdot\hat{\lambda}} \cdot\hat{\lambda}^{n \cdot \bar{y}}\\
\end{align}
$$

The first line just uses the power/exponential multiplication properties. The second line is just a writing simplification that is common in other resources, $mean(y)=\bar{y} = \frac{1}{n}\sum_{i=1}^n \Rightarrow n\cdot \bar{y} = \hat{\lambda}^{\sum^{n}_{i=1} y_i}$.

Now we want to find the prior distribution of $\hat{\lambda}$. First, we have to choose the distribution family of our prior. We will use a Gamma distribution. We are using this one because it lets the prior and the posterior have the same distribution family, it is called conjugate distributions. The prior is called a conjugate prior for the likelihood function. This means that the prior function and the likelihood function have the same form! And this means that we can simplify! let's try it :

$$
\begin{align}
[\hat{\lambda}]&\sim Gamma(\lambda,\alpha_p,\beta_p) \\
&= \lambda^{\alpha_p -1}\frac{\beta_p^\alpha\cdot e^{-\beta_p \lambda}}{\Gamma(\alpha_p)} \propto \lambda^{\alpha_p -1} \cdot e^{-\beta_p \lambda}
\end{align}
$$

Same as the likelihood, we are only interested in the term that varies with $\hat{\lambda}$ so we remove $\frac{\beta^\alpha}{\Gamma(\alpha)}$ and keep the proportional formula of the prior. We can now find the real formula of our posterior distribution.

$$ 
\begin{align}
[\lambda \mid y] &= [y \mid \lambda]\cdot[\lambda] \\
[\lambda \mid y] &\propto e^{-n \cdot\lambda} \cdot\lambda^{n \cdot \bar{y}} \cdot \lambda^{\alpha_p -1} \cdot e^{-\beta_p \lambda}\\
[\lambda \mid y] &\propto e^{-\beta_p \lambda-n\lambda} \cdot\lambda^{n\bar{y}+\alpha_p -1} \\
[\lambda \mid y] &\propto e^{-\lambda (\beta_p +n)} \cdot\lambda^{n\bar{y}+\alpha_p -1}\\
\end{align} 
$$

Does the last formula remind you of something familiar? That's right it is a Gamma distribution! This is the magic of the conjugate distributions. We can now write :

$$
\begin{align}
[\lambda \mid y] &\propto e^{-\lambda \beta} \cdot\lambda^{\alpha -1}\\
[\lambda \mid y] &\sim Gamma(\alpha_p +n\bar{y}, \beta_p +n)
\end{align}
$$

We can do some simulations to show the results. We are looking for the probability of $\hat{\lambda}$, so for the computation we create a vector of all $\hat{\lambda}$ for which we want to know the probability:

```{r}
lambda_hat <- seq(0,10, by = 0.01)
```

And now, in order to compare them, we compute the distribution of the prior and the posterior (which are both following a gamma distribution) with an increasing amount of sample:

```{r}
alph = 1
bet= 1

par(mfrow = c(2,3))
n_obs = 0
for(n_obs in list(0,1:5,1:10,1:50,1:100,1:200)){

  
  # prior distribution
  l_prior = dgamma(lambda_hat, shape = alph, rate = bet)
  
  # posterior distribution
  l_post = dgamma(lambda_hat, shape =  alph + sum(Y[n_obs]), rate = bet + max(n_obs))
  
  plot(lambda_hat,l_prior, ylim = c(0,max(c(l_post,l_prior))),
       type = 'l', lwd = 2, col = 'orange',
       xlab = expression(lambda), 
       ylab = expression(paste('[', lambda, '|y]')) )
  

  
  lines(lambda_hat,l_post, type = 'l',lty = 3, lwd = 2, col = 'purple')
  abline(v = 5, lty = 2, lwd = 2)
  
  title(paste("n = ",max(n_obs)))

}

```

When we add no data in the computation of the posterior, it is normal that we don't see any modifications from the prior. Adding 5 observation already bring some good information, the prior and the posterior have no longer the same shape and we have a better estimation of the true $\lambda$. Increasing the number of data gives us a better approximation of the true mean. The density of probability is also higher with a lot of data because we have more confidence in the approximation.

# When approximation by computation is needed

En statistiques bayésiennes, comme dit précédemment l'objectif est d'actualiser nos croyances sur les paramètres d'un modèle en combinant une connaissance a priori avec l'information observée dans nos données récoltées sur le terrain. Cependant, le calcul direct de la distribution postérieure peut souvent être compliqué à calculer analytiquement, surtout dans des modèles complexes. C'est là que les algorithmes Markov Chain Monte Carlo (MCMC) interviennent pour estimer les distributions a posteriori des paramètres.

### Small recap on Markov Chain

## Approximation with MCMC

Il existe de multiple algorithmes de type MCMC ( Metropolis-Hastings, échantillionneur de Gibbs, Hamiltonian Monte Carlo), chacun possèdes ses avantages et ses inconvénients et le choix de l'algorithme dépend de la situation d'application. Cependant ils partent tous de la même base théorique.

L'idée essentielle de la MCMC est que nous pouvons en apprendre davantage sur les paramètres non observés en effectuant de nombreux tirages aléatoires dans leurs distributions marginales a posteriori. L'accumulation de ces nombreux échantillons donne la "forme" de la distribution postérieure. Ces nombreux échantillons nous permettent de calculer les quantités qui nous intéressent : moyennes, variances et quantiles.

Il est maintenant légitime de se demander : Mais ça sert à quoi de tirer un échantillionnage dans une loi inconnu pour ensuite la reconstituer ? Et surtout comment on fait si on ne la connait pas ?

Et bien la loi que l'on reconstitue n'est pas totalement la même que celle qu'on utilise pour obtenir ces tirages. En effet la loi a posteriori n'est pas totalement inconnue. Ses informations sont portées par le produit de vraissemblance et du prior :

$$
[{\theta} \mid Y] \propto [Y \mid {\theta}  ]\cdot[{\theta}]
$$

C'est cette loi a priori estimée qui sert de première référence et le tirage aléatoire va permettre de normaliser cette expression pour qu'elle soit égal à un fonction de probabilité dont l'intégral est égal à 1. Cette propriété est normalement donnée par la division du produit par $[Y]$.

L'algorithme MCMC estime donc la loi a posteriori des paramètres non observé en évitant l'étape de calcul d'intégral qui peut s'avérer complexe pour obtenir $[Y]$.

**Step 1 : Initialisation**

### A small example without using JAGS

On reprend l'exemple avec les champignons.

```{r}
# [y | lambda]

likelihood = function(lambda,y){
  if(lambda < 0){
    return(0)
  }else{
    return( dpois(x = sum(y),lambda = lambda*length(y)))
  }
}

# [lambda]
prior.dist = function(lambda){
  dunif(lambda,0,1000)
}

# g([lambda_c(i) | lambda(i-1)])
dstep <- function(lambda1, lambda2, sd.explore = 0.1){
  dnorm(lambda2, mean = lambda1, sd = sd.explore)
}

rstep <- function(lambda, sd.explore = 0.1){
  rnorm(1, mean = lambda, sd = sd.explore)
}

MH.ratio <- function(lambda_c,lambda, y){
  ratio = (likelihood(lambda_c, y) * prior.dist(lambda) * dstep(lambda, lambda_c))/
    (likelihood(lambda, y) * prior.dist(lambda) * dstep(lambda_c, lambda))
  
  return(ratio)
}
```

```{r}
mcmc.function = function(lambda_init, n_iter = 10000, burning = 100, thin = 10){
  
  lambda_sample = rep(NA, n_iter)
  lambda_save = rep(NA, n_iter)
  lambda_sample[1] = lambda_init
  i=2
  for(i in 2:n_iter){
    lambda = lambda_sample[(i-1)]
    lambda_c = rstep(lambda)
    
    ratio = MH.ratio(lambda_c,lambda, Y)
    

    if(runif(1)<ratio){
      lambda_sample[i] = lambda_c
    }else{
      lambda_sample[i] = lambda
    }
    
  }
  lambda_save = lambda_sample[seq(burning,n_iter, by= thin)]
  return(data.frame(iteration = 1:length(lambda_save),
                    step = lambda_save))
}

```

```{r}
mcmc_df = mcmc.function(lambda_init = 10,burning = 200, n_iter = 1000000)

plot(step~iteration, mcmc_df, "l")
```

# IV) Prior Selection in Ecology

## IV-1) The role of prior knowledge and beliefs

In Bayesian statistics, prior knowledge and beliefs play a central role in the formulation and interpretation of Bayesian models. As explained before, the fundamentals of Bayesian inference lies in combining prior information with observed data to obtain updated or posterior probabilities.

Incorporating existing information to a data set can be based on previous studies, expert opinions, historical data or simply known subjective beliefs. It will allow the future model to avoid over-fitting and favor a more plausible and simple estimation.

## IV-2) Informative and non-informative priors

## IV-3) Incorporating expert opinions and literature data

# V) Case Studies in Bayesian Ecology

## Using JAGS

### two parameter example

For this example we are going to use data taken from a normal law. It is slightly different form the poisson distribution because normal distributions take 2 parameters. So we will need to estimate not only the mean but also the standard deviation will our bayesian approximation. We start by creating the data.

```{r}
Y2 = rnorm(n = 200, mean = 10, sd = 1)
hist(Y2)
```

For this example we will use the software *JAGS* with the library *rjgas*. *JAGS* is a software that let's us build our own model to estimate parameters that we want to find. It need 3 main componente. Fist, the model :

```{r}
mod1 = "
  model{
  ## model 
  for(i in 1:n_obs){# loop when there is multiple data. (will compute the product of the likelihood)
    Y2[i] ~ dnorm(m,preci) # likelihood of our data given parameters
  }
  
  ## Prior distribution
  # All parameters that we want to estimat need a prior distibution
  m ~ dunif(0, 10**3)
  sd ~ dunif(0, 10**3)
  preci <- 1/(sd*sd) # In JAGS dnorm take the precision as input not the standard deviation
  }
"
```

It look like *R* syntax but it is *JAGS* syntax, which is sligthly different for some function

Second, the list of data :

```{r}
data_list1 = list(
  Y2 = Y2,
  n_obs= length(Y2)
  
)
```

This list contain all information needed to make the model run.

Third, the list of initialization :

```{r}
init_list1 <- list(
  m = 5,
  sd = 2
)
```

*JAGS* will probably run MCMC algorithm to find the best parameters, those algorithm need values to start. Try to give values that make sense. If you don't the algorithm might take more time to converge or JAGS could also give error messages.

Now that every thing is setup we can run the model

```{r}
mjags1 <- jags.model(file=textConnection(mod1), 
                     data=data_list1,
                     inits=init_list1, 
                     n.chains = 3) # Number of MCMC 
```

```{r}
# function to extract mcmc 
postSamples1 <- coda.samples(mjags1, 
                             variable.names = c("m","sd"),
                             n.iter = 10000, 
                             thin = 10)

postSamples_df1 <-  postSamples1 %>% ggs()
```

Always verify if the markov chain are stationary (otherwise you will have a bad estimation of the posterior distribution)

```{r}
postSamples_df1%>%
  ggplot() +
  facet_wrap(~Parameter, scales = "free")+
  geom_line(aes(Iteration, value , col = as.factor(Chain)), alpha = 0.3)+
```

All MCMC seem to be stationary, we can go on.

```{r}
scale_color_manual(values = wesanderson::wes_palette("FantasticFox1", n = 5))          
postSamples_df1 %>%
  ggplot() +
  facet_wrap(~as.factor(Chain)*Parameter, scales = "free") +
  geom_histogram(data = postSamples_df1, aes(x= value, y =..density.., fill = as.factor(Chain)),
                 position = "identity", alpha = 0.7)+
  scale_fill_manual(values = wesanderson::wes_palette("FantasticFox1", n = 5))

str(postSamples_df1$value)
# postSamples_df1 %>%
#   pivot_wider(id_cols = -c(Iteration, Chain), names_from = "Parameter", values_from = "value")%>%
#   ggplot() +
#   facet_wrap(~as.factor(Chain), scales = "free") +
#   geom_point(data = postSamples_df1, aes(x= m, y =sd),
#                  position = "identity", alpha = 0.7)

```

Here we look at the posterior distribution estimated by the 3 chain for the mean and the standard error.

For this example we are going to use data taken from a normal law. And we can for example suppose that we want to study the length of snakes in 5 different dessert (that we gonna call A,B,C,D,E for simplicity).

```{r}

set.seed(1000)
y_means = round(runif(n= 5, min = 5, max = 15))
n_obs = 40
Y3 = c()
class = c()
IDclass = c()
ID = c("A","B","C","D","E")
for(i in 1:length(y_means)){
 Y3= c(Y3, rnorm(n = n_obs, mean = y_means[i], sd = 2))
 class = c(class,rep(ID[i],n_obs))
 IDclass = c(IDclass,rep(i,n_obs))
}
data = data.frame(Y3, class, IDclass)

ggplot(data)+
  geom_sina(aes(class,Y3))

```

```{r}
mod2 = "
  model{
  ## model 
  for(i in 1:n_obs){
    Y3[i] ~ dnorm(m[IDclass[i]],preci)
  }
  
  ## Prior distribution
  for(n in 1:n_class){
    m[n] ~ dunif(0, 10**3)
  }
  sd ~ dunif(0, 10**3)
  preci <- 1/(sd*sd)
  }
"
```

For this time we want to estimate 5 different means and we will suppose that standard deviation is the same between each class (but we could also estimate it with respect to the class). To estimate multiple parameter we initiate as much prior as there is class with the loop `for(n in 1:n_class){ m[n] ~ dunif(0, 10**3)}`

```{r}
data_list2 = list(
  Y3 = data$Y3, # data vector
  n_obs= length(data$Y3), # number of observations
  IDclass = data$IDclass, # vector to specify class of each observation
  n_class = max(data$IDclass) # Number of class
)
```
For this example we add `IDclass = data$IDclass` and ` n_class = max(data$IDclass)` for the class specification in the model.

```{r}
init_list2 <- list(
  m = rep(5,5), 
  sd = 2
)
```

This is where we initiate the prior, the only change is that we initialize 5 mean with `m = rep(5,5)`

We run the model :
```{r}
mjags2 <- jags.model(file=textConnection(mod2), 
                     data=data_list2,
                     inits=init_list2, 
                     n.chains = 3)

postSamples2 <- coda.samples(mjags2, 
                             variable.names = c("m","sd"),
                             n.iter = 10000, 
                             thin = 10)
```

Some data manipulation for the visualization
```{r}
postSamples_df2 <-  postSamples2 %>% ggs()
params_names = unique(postSamples_df2$Parameter)
postSamples_df2$true_values = NA
for( i in 1:6){
  temp = which(postSamples_df2$Parameter == params_names[i])
  if(substr(params_names[i],1,1) == "m"){
      postSamples_df2$true_values[temp] = y_means[i]
  }else{
    postSamples_df2$true_values[temp] = 2
  }

}
```

Check if the chains are stationary.
```{r}
postSamples_df2%>%
  ggplot() +
  facet_wrap(~Parameter, scales = "free")+
  geom_line(aes(Iteration, value , col = as.factor(Chain)), alpha = 0.3)+
  scale_color_manual(values = wesanderson::wes_palette("FantasticFox1", n = 5)) 
```

Finally we look at the results of posterior distribution of the estimated means and the estimated standard deviation.
```{r}
         
postSamples_df2 %>%
  ggplot() +
  facet_wrap(~Parameter, scales = "free") +
  geom_histogram(aes(x= value, y =after_stat(density), fill = as.factor(Chain), col = as.factor(Chain)),
                 position = "identity", alpha = 0.3)+
  geom_histogram(aes(x= value, y =after_stat(density), col = as.factor(Chain)),
                 position = "identity", alpha = 0)+
  scale_fill_manual(values = wesanderson::wes_palette("FantasticFox1", n = 5))+
  scale_color_manual(values = wesanderson::wes_palette("FantasticFox1", n = 5))+
  geom_vline(aes(xintercept = true_values), alpha = 0.7, linetype = "dashed")

# str(postSamples_df1$value)
# postSamples_df1 %>%
#   pivot_wider(id_cols = -c(Iteration, Chain), names_from = "Parameter", values_from = "value")%>%
#   ggplot() +
#   facet_wrap(~as.factor(Chain), scales = "free") +
#   geom_point(data = postSamples_df1, aes(x= m, y =sd),
#                  position = "identity", alpha = 0.7)

```

## V-1) Highlighting specific ecological studies that employed Bayesian methods

## V-2) Illustrative examples from different ecological sub-disciplines
