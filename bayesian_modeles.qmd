---
title: "Introduction to bayesian modeles for ecology"
author : Coline - Jules - Mathis - Romane - Laura
bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

```{r setup}
#| include: false

library(tidyverse)
library(deSolve)

```

# Introduction

-   History of Bayesian concept.
-   Philosophy behind it. Frequentist approach : "What is the probability to observe my data given my model."

Bayesian approach : "What is the probability of m y model given my data."

Rappel : - différence entre loi à posterio jointes et marginales

# General theory behind Bayesian approach.

Explanation in the special case of conjugation between prior and posterior

## Main objective, for context

First, let's take an simple example to explain the idea behind Bayesian method. We want to estimate the mean abundance per $m^2$ of one fungus in a forests. To do that, we set up it some sampling area in which we count the number of mushroom. We can simulate the data by taking random observations in a poisson distribution. Let's suppose that our studies as 200 sampling area. **Because it is count data, number of mushroom that we will count should follow a poisson distribution :**

```{r}
n_sample = 200 # sampling area
lbda = 5 # mean of poisson distribution
pois_distr = dpois(1:20, lambda = lbda)
plot(pois_distr,type ="h",
     lwd = 2, col = 'blue',
     xlab = "Mushroom Count", 
     ylab = expression(paste( 'Density or ','[y]')) )

```

To simulate the sampling campaign we are taking values in this poisson distribution . In our case those will be used to estimate the mean number of mushroom in the forest.($\lambda = 5$ is already known because we simulate the data, but in reality this is an unknown).

```{r}
set.seed(1000)

Y = rpois(n = n_sample, lambda = lbda)

hist(Y)
```

**So let's pretend that we don't know the** $\lambda$. We want to find the the value of the mean number of mushroom we will call $\hat{\lambda}$ and we also want to know the probability of this $\hat{\lambda}$. In fact the Bayesian method will give us the multiple estimated mean $\hat{\lambda}$ and the probability of those value to be true knowing the observation $Y$. We can simply write this as follow $[\hat{\lambda} \mid Y]$ which is the probability of hat lambda knowing our observations. This probability can be found with the equation :

$$
[\hat{\lambda} \mid Y] = [Y \mid \hat{\lambda}  ]\cdot[\hat{\lambda}]
$$

The two component of the right hand side of the equation are $[Y \mid \hat{\lambda} ]$ the likelihood of our data and $[\hat{\lambda}]$ the prior distribution. First lets start with the likelihood. Our data follow a poisson distribution so our likelihood will follow a poisson distribution :

$$
L(\hat{\lambda} ; y)=[y \mid \hat{\lambda}] = \frac{e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y}}{y!}
$$

This a first good step, but there is a small issue here, this formula isn't completely usable in this form. This is because it can only take one observation. In english words it is like asking what is the probability of one observation (one count of mushroom) given a model with a mean $\hat{\lambda}$. It it clear that this form isn't powerful enough because it use only one observation. What we want is to use all the data that we have, we want to know the probability of all the observations given a model with a mean $\hat{\lambda}$. To do so we can write the likelihood of all our data $Y$ as the product of the likelihood of each observation $y_i$. (We are allowed to do this only because observations are independent)

$$
\begin{align}
[Y \mid \hat{\lambda}] &= \prod^{n}_{i=1}[y_i \mid \hat{\lambda}] \\
&=\prod^{n}_{i=1}\frac{e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y_i}}{y_i!} \\
&\propto \prod^{n}_{i=1}e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y_i}\\
\end{align}
$$

As you can see we don't keep the $\frac{1}{y_i!}$, it is because we are only interested in the terms that are impacted by $\hat{\lambda}$. The last form which is proportional to the likelihood function have a more convenient form for the next step. Lets rearrange the function in a more convenient form

$$
\begin{align}
\prod^{n}_{i=1}e^{-\hat{\lambda}} \cdot\hat{\lambda}^{y_i}&= e^{-n \cdot\hat{\lambda}} \cdot\hat{\lambda}^{\sum^{n}_{i=1} y_i}\\
&= e^{-n \cdot\hat{\lambda}} \cdot\hat{\lambda}^{n \cdot \bar{y}}\\
\end{align}
$$

The fist line just use the power/exponential multiplication properties. The second line is just a writing simplification that is common in other resources, $mean(y)=\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i \Rightarrow n\cdot \bar{y} = \hat{\lambda}^{\sum^{n}_{i=1} y_i}$.

Now we want to find the prior distribution of $\hat{\lambda}$. Fist we have to choose the distribution family of our prior. We will use a Gamma distribution. We are using this one because it let the prior and the posterior have the same distribution family, it is called conjugate distributions. The the prior is called a conjugate prior for the likelihood function. Which mean that the prior function and the likelihood function have the same form ! And this means that we can simplify ! lets try it :

$$
\begin{align}
[\hat{\lambda}]&\sim Gamma(\lambda,\alpha_p,\beta_p) \\
&= \lambda^{\alpha_p -1}\frac{\beta_p^\alpha\cdot e^{-\beta_p \lambda}}{\Gamma(\alpha_p)} \propto \lambda^{\alpha_p -1} \cdot e^{-\beta_p \lambda}
\end{align}
$$

Same as the likelihood, we are only interested by the term that vary with $\hat{\lambda}$ so we remove $\frac{\beta^\alpha}{\Gamma(\alpha)}$ and keep the proportional formula of the prior. We can now find real formula of our posterior distribution.

$$ 
\begin{align}
[\lambda \mid y] &= [y \mid \lambda]\cdot[\lambda] \\
[\lambda \mid y] &\propto e^{-n \cdot\lambda} \cdot\lambda^{n \cdot \bar{y}} \cdot \lambda^{\alpha_p -1} \cdot e^{-\beta_p \lambda}\\
[\lambda \mid y] &\propto e^{-\beta_p \lambda-n\lambda} \cdot\lambda^{n\bar{y}+\alpha_p -1} \\
[\lambda \mid y] &\propto e^{-\lambda (\beta_p +n)} \cdot\lambda^{n\bar{y}+\alpha_p -1}\\
\end{align} 
$$

Do the last formula remind you something familiar ? That's right it is a Gamma distribution ! This is the magic of the conjugate distributions. We can now write :

$$
\begin{align}
[\lambda \mid y] &\propto e^{-\lambda \beta} \cdot\lambda^{\alpha -1}\\
[\lambda \mid y] &\sim Gamma(\alpha_p +n\bar{y}, \beta_p +n)
\end{align}
$$

We can do some simulations to show the results. We are looking for the probability of $\hat{\lambda}$, so for the computation we create a vector of all $\hat{\lambda}$ for which we want to know the probability:

```{r}
lambda_hat <- seq(0,10, by = 0.01)
```

And now, in order to compare them, we compute the distribution of the prior and the posterior (which are both following a gamma distribution) with an increasing amount of sample:

```{r}
alph = 1
bet= 1

par(mfrow = c(2,3))
n_obs = 0
for(n_obs in list(0,1:5,1:10,1:50,1:100,1:200)){

  
  # prior distribution
  l_prior = dgamma(lambda_hat, shape = alph, rate = bet)
  
  # posterior distribution
  l_post = dgamma(lambda_hat, shape =  alph + sum(Y[n_obs]), rate = bet + max(n_obs))
  
  plot(lambda_hat,l_prior, ylim = c(0,max(c(l_post,l_prior))),
       type = 'l', lwd = 2, col = 'orange',
       xlab = expression(lambda), 
       ylab = expression(paste('[', lambda, '|y]')) )
  
  
  lines(lambda_hat,l_post, type = 'l',lty = 3, lwd = 2, col = 'purple')
  abline(v = 5, lty = 2, lwd = 2)
  
  title(paste("n = ",max(n_obs)))

}

```

When we add no data in the computation of the posterior, it is normal that we don't see any modifications from the prior. Adding 5 observation already bring some good information, the prior and the posterior have no longer the same shape and we have a better estimation of the true $\lambda$. **Increase the number of data give us a better approximation of the true mean. The density of probability is also higher with a lot fo data, because we have more confidence in the approximation.**

# When approximation by computation is needed

In Bayesian statistics, as mentioned earlier, the goal is to update our beliefs about the parameters of a model by combining prior knowledge with the observed information in our collected field data. However, the direct calculation of the posterior distribution can often be challenging to compute analytically, especially in complex models. Resorting to conjugate distribution laws is one solution to overcome the problem, but this is not always feasible. This is where Markov Chain Monte Carlo (MCMC) algorithms come into play to estimate the posterior distributions of parameters.

There are multiple MCMC algorithms (Metropolis-Hastings, Gibbs sampler, Hamiltonian Monte Carlo), each with its own advantages and disadvantages, and the choice of the algorithm depends on the application scenario. However, they all share the same theoretical foundation.

The essential idea of an MCMC algorithm is to create a sequence of dependent random numbers via a Markov chain. When the chain reaches equilibrium (stationary state), it forms a sampling within the posterior distribution, allowing the calculation of quantities of interest such as means, variances, quantiles, and so on.


## Implémentation de l'algorithm de Metropolis-Hastings

Let's implemant the algortihm to try to understand how it works. In the following lines, we will describe the various steps of the Metropolis-Hastings algorithm along with the corresponding R code lines.

**#Step 1 : Definition of the likelihood function and the prior law**

As with the beginning of any Bayesian statistical analysis, we first pose the problem and define the likelihood probability distribution and the prior distributions of the parameters to be estimated.

We define the likelihood as a binomial distribution with parameters $N$ as the total population size and $p$ as the probability of capturing individuals. It is this parameter $p$ that we aim to estimate. We make the assumption that we have no prior information about this parameter, so we choose a non-informative prior:

$$
p \sim \beta(1,1)
$$

```{r}
# [y | p]

likelihood<- function(p, y, N){
  if(p < 0 | p > 1) { 
    return(0)
  } else {
    return(dbinom(x = y, size = N, prob = p))
  }
}

```

```{r}
# [p]
prior.dist = function(p, a.prior = 1, b.prior = 1){
  dbeta(x = p, shape1 = a.prior, shape2 = b.prior)
}
```

**Step 2 : Definition of a candidate position**

We will now aim to move, so we need to define a function to determine a candidate position. Here, we randomly draw this position from a normal distribution with a mean of $p_c$, which is the current position, and we arbitrarily choose a standard deviation value.

```{r}

move  <- function(p, sd.explore = 0.1){
  candidate <- rnorm(1, mean = p, sd = sd.explore)
  return (candidate)
}

```

**Step 3 : Compute of the ratio**

Once we have this candidate position, we need to decide whether to keep it or not. The decision criterion to calculate is the Metropolis-Hastings ratio $r$, which we define as:

$$
r = \dfrac{[p_{t+1} \mid Y]\space \cdot \space [p_{t+1}] \space \cdot \space g(p_{t+1} \mid p_{t})}{[\lambda_{t} \mid Y]\space \cdot \space [p_{t}] \space \cdot \space g(p_{t} \mid p_{t+1})}
$$

where $g(p_{t+1} \mid p_{t})$ is the probability of transitioning from the candidate position to the current position.

```{r}

proba_move <- function(p1, p2, sd.explore = 0.1){
  dnorm(p1, mean = p2, sd = sd.explore)
} 

```

```{r}
MH.ratio <- function(p_c,p, y, N){
  ratio = (likelihood(p_c, y, N) * prior.dist(p) * proba_move(p, p_c))/
    (likelihood(p, y, N) * prior.dist(p) * proba_move(p_c, p))
  
  return(ratio)
}
```

**Step 4 : Decide if we go to the candidate position or not**

To choose if we keep the candidate position $\lambda_{t+1}$ we define $u$ : 

$$
u \sim unif(0,100)
$$

If $u$ is greater than the ratio $r$, we remain at the current position; conversely, if $u$ is less than $r$, we transition to the candidate position.

```{r}
# parameter algorithm
n_iter = 1000
thin = 10

#data 
y = 3
N = 10

#initialization 
p_init = 0.5
p_sample = rep(NA, n_iter)
p_save = rep(NA, n_iter)
p_sample[1] = p_init
i=2


for(i in 2:n_iter){
  p = p_sample[(i-1)]
  p_c = move(p)
    
  ratio = MH.ratio(p_c,p, y, N)
    

  if(runif(1)<ratio){
    p_sample[i] = p_c
  }else{
    p_sample[i] = p
  }
    
  }
p_save = p_sample[seq(1,n_iter, by= thin)]

data = data.frame(iteration = 1:length(p_save),
                    step = p_save)


```


There is a parameter we haven't discussed in this function, and that's $thin$. It means that we will only save the samples within the chain at intervals of 
$thin$. This is due to the fact that the samples are correlated with each other as they depend on the previous position and therefore do not accurately reflect the distribution. Consequently, if we want independent samples, we must discard the majority of samples and keep only one sample every 
$thin$ steps, with 
$thin$ being "sufficiently large."

**Step 5 : Visualization**

```{r}
plot(step~iteration, data, "l")

p1 <- ggplot(data = data)+
  geom_histogram(aes(x = step))+
  theme_bw()
p1

alph = 1
bet= 1

```

Both graphs represent the sampling of $p$ in its marginal posterior distribution. This is possible when a sufficient number of iterations is used, allowing the Markov chain to reach a stationary state.

## Discussion with two parameters

Et comment ça se passe dans plusieurs dimensions ? Evidemment au dessus de deux paramètres à estimer la visualisation devient impossibile. Donc pour que cela reste visuel, on va seulement réaliser un exemple avec un paramètre de plus à estimer. 

We will use a simple example: estimating a population mean and standard deviation. We’ll define some population level parameters, collect some data, then use the Metropolis-Hastingsalgorithm to simulate the joint posterior of the mean and standard deviation.

On commence par simuler des données.

```{r}
library(sm)

# population level parameters
mu <- 7
sigma <- 3

# collect some data (e.g. a sample of heights)
n <- 50
x <- rnorm(n, mu, sigma)

```
Puis comme précédemment on définit la vraissemblance et les lois a priopri des paramètres à estimer : mu et sigma : 

$$
Y \sim N(\mu,\sigma)
$$
```{r}
# likelihood function
ll <- function(x, muhat, sigmahat){
  sum(dnorm(x, muhat, sigmahat, log = T))
}
```
$$
\mu \sim N(0,100)
$$
$$
\sigma \sim unif(0,10)
$$
```{r}
# prior
pmu <- function(mu){
  dnorm(mu, 0, 100, log = T)
}

psigma <- function(sigma){
  dunif(sigma, 0, 10, log = T)
}

```

```{r, results='hide'}

geninits <- function(){
  list(mu = runif(1, 4, 10),
       sigma = runif(1, 2, 6))
}

# Probabilité de transition multivariée (loi normale)
transition_prob <- function(theta_candidate, theta_current, proposal_sd = 0.1) {
  dnorm(theta_candidate, mean = theta_current, sd = proposal_sd, log = TRUE)
}

post <- function(x, mu, sigma){
  ll(x, mu, sigma) + pmu(mu) + psigma(sigma)
}

jump <- function(x, dist = .2){ # must be symmetric
  x + rnorm(1, 0, dist)
}
iter = 10000
chains <- 3
posterior <- data.frame(mu = rep(NA,iter), sigma = rep(NA,iter))
accepted <- c(rep(NA,iter-1))

set.seed(1234)

theta.post <- data.frame(mu = rep(NA,iter), sigma = rep(NA,iter))

theta.post[1, 1] <- 9
theta.post[1, 2] <- 5
  t =2
for (t in 2:iter){
    # theta_star = proposed next values for parameters
    theta_star <- c(jump(theta.post[t-1, 1],0.1),jump(theta.post[t-1, 2],0.1))
    
    #cacul ratio => à changer 
    pstar <- post(x, mu = theta_star[1], sigma = theta_star[2])  
    pprev <- post(x, mu = theta.post[t-1, 1], sigma = theta.post[t-1, 2])
    r <- (pstar+transition_prob(c(theta.post[t-1,1],theta.post[t-1,2]), theta_star, 0.1) )-(pprev+transition_prob(theta_star, c(theta.post[t-1,1],theta.post[t-1,2]), 0.1))
    ratio = exp(r[1])
    # Exemple d'utilisation

    # theta_star is accepted if posterior density is higher w/ theta_star
    # if posterior density is not higher, it is accepted with probability r
    # else theta does not change from time t-1 to t
    accept <- rbinom(1, 1, prob = min(r, 1))

    if(runif(1)<ratio){
      theta.post[t, ] <- theta_star
    } else {
      theta.post[t, ] <- theta.post[t-1, ]
    }
  
  posterior <- theta.post
}

```



```{r}

sequence <- unique(round(exp(seq(0, log(iter), length.out = 150))))

xlims <- c(4, 10)
ylims <- c(1, 6)

  par(mfrow=c(1, 2))
  plot(posterior[ 1:1000,1], posterior[1:1000,2],
       type="l", xlim=xlims, ylim=ylims, col="blue",
       xlab="mu", ylab="sigma", main="Markov chains")
  text(x=7, y=1.2, paste("Iteration ", 1000), cex=1.5)
  sm.density(x=cbind(c(posterior[1:1000,1]), c(posterior[1:1000,2])),
             xlab="mu", ylab="sigma",
             zlab="", zlim=c(0, .7),
             xlim=xlims, ylim=ylims, col="white", 
             verbose=0)
  title("Posterior density")

```

## Some words about Rjags

After implementing the Metropolis-Hastings algorithm from scratch, it's time to let you know that there are tools available to perform all of this more conveniently. One commonly used tool is RJags. As stated in its documentation, "The rjags package provides an interface from R to the JAGS library for Bayesian data analysis. JAGS uses Markov Chain Monte Carlo (MCMC) to generate a sequence of dependent samples from the posterior distribution of the parameters."

However, the model definition (likelihood, prior, process model, etc.) has its peculiarities. It is done in a separate text file, subsequently called in R, and is written in the BUGS language, which happens to be very close to R. To avoid any confusion about syntax, we recommend referring to the documentation for more details.
# IV) Prior Selection in Ecology

## IV-1) The role of prior knowledge and beliefs

In Bayesian statistics, prior knowledge and beliefs play a central role in the formulation and interpretation of Bayesian models. As explained before, the fundamentals of Bayesian inference lies in combining prior information with observed data to obtain updated or posterior probabilities.

Incorporating existing information to a data set can be based on previous studies, expert opinions, historical data or simply known subjective beliefs. It will allow the future model to avoid over-fitting and favor a more plausible and simple estimation.

## IV-2) Informative and non-informative priors

## IV-3) Incorporating expert opinions and literature data

# V) Case Studies in Bayesian Ecology

## V-1) Highlighting specific ecological studies that employed Bayesian methods

## V-2) Illustrative examples from different ecological sub-disciplines
