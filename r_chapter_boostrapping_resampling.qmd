---
title: "Chapter : Bootstrapping and Resampling"

bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

# Packages

```{r, result = F, message = F}
library(tidyverse)
library(latex2exp)
library(datasets)
```

# General introduction

A dataframe:

```{r}
data <- iris
head(iris)
```

## Confidence interval on a sample

Peut-on estimer la taille moyenne

```{r}

hist(data$Sepal.Width)
shapiro.test(data$Sepal.Width)
qqnorm(data$Sepal.Width)
qqline(data$Sepal.Width)
#Or iris dataset ? 
```

L'échantillon est normalement distribué, on peut donc calculer l'intervalle de confiance de la moyenne de la manière suivante:

```{r}
low <- mean(data$Sepal.Width)-1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
high <- mean(data$Sepal.Width)+1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
print(c(low,high))
```

rq: source INSEE taille moyenne F = 1 m 62, et https://ourworldindata.org/grapher/average-height-by-year-of-birth?time=latest&country=\~FRA taille moyenne = 1m6488 pour les gens nés en 1996 (pas de date après) Not in intervall, pop grow, + ech non représentatif de la pop française

## Confidence interval on multiple sub-samples means

Si on veut désormais estimer $\mu$, la taille moyenne des étudiants sur les trois promotions du jeu de données.

```{r}
(mu <- mean(data$Sepal.Width))
```

mu correspond à la taille moyenne qu'on veut estimer mais dont on ne connait pas la véritable valeur.

# The Jackknife resampling method

Imaginons mantenant que nous ayons seulement accès à un échantillon et qu'on veuille estimer la taille moyenne des étudiants de ces trois promotions.

```{r}
set.seed(121)
data_sample <- data[sample(1:150,30),]
mean(data_sample$Sepal.Width)
hist(data_sample$Sepal.Width)
shapiro.test(data_sample$Sepal.Width)
qqnorm(data_sample$Sepal.Width)
qqline(data_sample$Sepal.Width)
```

On a seulement accès à cet échantillon non normalement distribué. La dessus, nous ne pouvons pas calculer les intervalles de confiance comme vu précedemment. C'est là que je Jackknife devient intéressant.

## a - Principe

## b - exemple d'implémentation

test pas du tout fini

```{r}
group <- c()
sampled_data <- c()
for (i in 1:20) { #au total, 100 individus auront été échantillonnés
  set.seed(i)
  sample_temp <- sample(setdiff(1:139,sampled_data),5)
  sampled_data <- c(sampled_data,sample_temp)
  group <- c(group,mean(data[sample_temp,"Sepal.Width"]))
  
}
```

```{r}
hist(group)
qqnorm(group)
qqline(group)
shapiro.test(group)
```

## c - Avantages et inconvénients de la méthode

# Permutation Tests

Permutation tests represent a powerful statistical method for comparing groups and assessing the influence of one variable on another. They aimed to produce statistical inferences and a p-value. In contrast to parametric tests, permutation tests do not rely on specific assumptions about the data distribution, making them more robust in certain situations especially when the number of replicates is low. Furthermore, even if the data in our sample does not adhere to a classical distribution (e.g., Gaussian), statistical inference remains feasible. These tests operate intuitively, are easy to implement, and their robustness makes them particularly practical in numerous scenarios.

## Foundations of Permutation Tests

### Fundamental Principle

Similar to other classical statistical tests (e.g., Student's t-test, $\chi^2$ test), permutation tests are grounded in the distribution of a test statistic under the null hypothesis. The distribution under the null hypothesis corresponds to the theoretical distribution of the test statistic, assuming that the tested effect has no impact (i.e., explanatory variables have no effect on the response variable). In classical statistical tests, this distribution aligns with various known laws, with the most recognized being the Gaussian distribution, although it could also be the Student's t-distribution, the $\chi^2$ distribution, etc. The distinctiveness of permutation tests lies in empirically generating this theoretical distribution directly from the data, putting them in the category of the **exact tests** et **non-parametrics tests**.

According to the central limit theorem, repeating a large number of observations of a random variable (here, the test statistic) leads to a Gaussian distribution. This distribution allows us to position the observed test statistic (calculated on our sample) and determine the significance of the effect of our explanatory variables. In other words, we generate a large number of permutations of the observations, calculate the test statistic for each permutation, and then compare the observed statistic to this null distribution.

### **Parametric tests** : test statistic, null distribution and p-value

The test statistic is a numerical measure used to evaluate whether observed differences between groups in a study are statistically significant. It quantifies the gap between observed and randomly attributable observations. Here are some examples of well-known test statistics:

$$
\begin{align*}
F [\text{ANOVA}] &: \quad F = \frac{MS_{\text{between}}}{MS_{\text{within}}} \quad \sim \mathcal{F}(df_{\text{between}}, df_{\text{within}}) \\
\\
t [\text{Student's t-test}] &: \quad t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \quad \sim t(df) \\
\\
\chi^2 [\text{$\chi^2$ Test}] &: \quad \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i} \quad \sim \chi^2(df) \\
\end{align*}
$$

With $MS$ as the mean square error, $df$ as the degrees of freedom, $\bar{X}_i$ as the mean of group $i$, $s^2_i$ as the variance of group $i$, $n_i$ as the number of replicates in group $i$, $O_i$ as the observed $\chi^2$ distance, and $E_i$ as the expected $\chi^2$ distance.

These statistics usually follow known distributional laws, allowing us to compare the observed statistic (calculated on the data sample) to the null distribution of that statistic and obtain a p-value by comparing the expected values of the statistic under the null hypothesis with the observed statistic obtained with our data. To illustrate, let's say we want to compare the sepal length between the species *I. virginica* and *I. versicolor* using a Student test. First, we plot the data :

```{r, echo = F, out.width="50%", fig.show='hold'}
par(mfrow = c(1, 2))
boxplot(iris$Sepal.Length[iris$Species == "virginica"], col = rgb(0, 0, 1, alpha = .5), ylab = "Sepal Length", xlab = "", main = "Virginica", ylim = c(4, 8))
boxplot(iris$Sepal.Length[iris$Species == "versicolor"], col = rgb(1, 0, 0, alpha = .5), ylab = "Sepal Length", xlab = "", main = "Versicolor", ylim = c(4, 8))
```

*I. virginica* seems to have slightly bigger sepal length than *I. versicolor*. Assuming all conditions are met, we perform a Student test to determine the significance of the difference observed :

```{r}
t.test(
  x = iris$Sepal.Length[iris$Species == "virginica"],
  y = iris$Sepal.Length[iris$Species == "versicolor"],
  paired = F,
  var.equal = T,
  alternative = "two.sided"
)

```

Here we observe a significance difference between the species *I. virginica* and *I. versicolor* with *I. virigina* having bigger sepal length than *I. versicolor*.

Let's get more precise on how do we get the p-value associated with this comparison. The Student distribution under null hypothesis is defined according the degree of freedom, here equal to $df = 98$. Let's plot this $H0$ distribution density of our test :

```{r, fig.height=4, fig.width=4}
# Density distribution of the t statistic
curve(dt(x = x, df = 98), xlim = c(-6, 6), ylab = "Density", xlab = "t")
```

This density distribution represents the probability associated for each value of the $t$ statistic for a degree of freedom of $98$. Now, we have to place in this distribution the $t_{obs}$ calculated from our data, here $t_{obs} = 5.6$. Knowing we have performed a two-sided test (i.e. no *a priori* of the direction of the effect, i.e. the sepal length could have been bigger either for one species or the other), we'll place visually the values of $t$ for which less than $2.5\%$ and more than $97.5\%$ of values are respectively below and above these thresholds. If our $t_{observed}$ falls below the $2.5\%$ or above $97.5\%$ threshold, the difference observed is considered statistically significant.

```{r, fig.height=4, fig.width=4}
# Density distribution of the t statistic
curve(dt(x = x, df = 98), xlim = c(-6, 6), ylab = "Density", xlab = "t", lwd = 1.5)

# Quantile function : values of t associated of a two sided test
threshold <- qt(p = c(0.025, 0.975), df = 98)
abline(v = c(threshold[1], threshold[2]), lwd = 1.5, lty = 2, col = "red")


# Color the area of significance
x <- seq(-6, 6, length.out = 100000)
y <- dt(x, df = 98)
polygon(c(x[x<=threshold[1]], threshold[1]), c(y[x<=threshold[1]], y[x == max(x)]), col = "red")
polygon(c(x[x>=threshold[2]], threshold[2]), c(y[x>=threshold[2]], y[x == max(x)]), col = "red")


# Placing the t_obs value
abline(v = 5.6, lwd = 1.5, col = "blue")
text(x = 4.5, y = 0.2, labels = TeX("$t_{obs}$"), col = "blue")
```

Hence $t_{obs}$ is in the significance area (in red) meaning the difference is significant.

### Permutation test : test Statistic, null distribution and p-value

The test statistic for permutation tests depends on the type of study and the relationship between the data. In permutation test, the statistic test $P_{obs}$ have to be defined. For a test of the difference in means between two groups, for example, the test statistic could be defined as follows:

$$
P = \bar{X}_1 - \bar{X}_2
$$

where ($\bar{X}_1$) and ($\bar{X}_2$) are the means of the two groups compared. But where do we place this $P_{obs}$ value ? We have to construct the null distribution. To do so, we will permute randomly each value of both group in one group of each other and calculate the test statistic $P_{H0}$ for this new data set. In other word, each data could randomly be assigned either in group 1 or group 2, characterizing the absence of effect of belonging to one group or the other for the response variable as data has been shuffled randomly. We'll do this procedure a great amount of time and each time extract the $P_{H0}$ obtained. We'll then get a distribution of the test statistic under $H0$ with all the $P_{H0}$ obtained during the iterations. According to the central limit theorem stating the distribution of random variables repeated a large number of time will end up following a bell curve distribution, no matter what shape the original distribution of those variable have. Once we obtained this null distribution, we'll simply follow the same procedure as describe above for the parametric tests and compare our $P_{obs}$ to this null distribution and determine its probability. In permutation test, the p-value corresponds to the number of iteration of $P_{H0}$ above or below (depending of the direction of our hypothesis) the $P_{obs}$ value.

## Implementation of Permutation Tests

### Test Steps

Typical steps to implement a permutation test include:

1.  Formulate null and alternative hypotheses
2.  Determine and calculate the test statistic from the observed data
3.  Generate a large amount of permutations of the data
4.  Calculate the test statistic for each permutation
5.  Compare the observed statistic to the null distribution of test statistics
6.  Conclude on accepting or rejecting the null hypothesis

### Practical examples

#### Comparison of means

Consider an example where we want to test if the difference in means between two groups is statistically significant. We'll use the statistic for means described above and use the same question asked for the Student test example : "Is the sepal length of the genra *I. virginica* and *I. versicolor* different ?" but only with 20 random chosen observations.

```{r}
set.seed(123)

# SELECT THE VARIABLES OF INTEREST
filter_data <- iris %>% 
  filter(Species == "virginica" | Species == "versicolor") %>% 
  droplevels()

# TAKE A SUBSET
sub_data <- filter_data[sample(1:nrow(filter_data), size = 20),]
```

Let's plot the sub-dataset.

```{r, echo = F, out.width="50%", fig.show='hold'}
hist(sub_data$Sepal.Length[sub_data$Species == "virginica"], col = rgb(0, 0, 1, alpha = .5), ylab = "Frequency", xlab = "Sepal Length", main = "Virginica", ylim = c(0, 5))
hist(sub_data$Sepal.Length[sub_data$Species == "versicolor"], col = rgb(1, 0, 0, alpha = .5), ylab = "Frequency", xlab = "Sepal Length", main = "Versicolor")
```

Here we can clearly see that the data does not follow a Gaussian distribution like we could have expect with such a few dataset. Furtheremore, neither of the test for comparison of means show a significant differences between sepal length of the two species neither with a Student test or the Mann-Whitney non-parametrical test (note that we used a Student test here even if the normal condition is not met. This serves as illustration and comparison with previous test with the full dataset).

```{r, warnings = F}
# T TEST
t.test(
  x = sub_data$Sepal.Length[sub_data$Species == "virginica"],
  y = sub_data$Sepal.Length[sub_data$Species == "versicolor"],
  paired = F,
  var.equal = T,
  alternative = "two.sided"
)

# MANN-WHITNEY TEST
wilcox.test(sub_data$Sepal.Length[sub_data$Species == "virginica"],
  sub_data$Sepal.Length[sub_data$Species == "versicolor"],
  paired = F)
```

This is where permutation test becomes interesting. They have a great power even when the number of replicates is low. Let's see how we proceed.

Permuted data can be generated (among many other ways) by randomly assign levels of the variable 'species' to sepal length values using the `sample` function. Then, the difference of means will be computed to calculate `p_h0` the statistic for the permuted data (the difference of means of sepal length of the permuted data). We'll do this for $i$ `iterations` yielding $i$ $P_{H0}$ resulting in the `null_distribution`.

```{r}
# INITIAL PARAMETERS
iteration <- 10000  # number of iteration
null_distribution <- c()  # storage of the statistic for permuted data


# ALGORITHM
for (i in 1:iteration)
{
  # --- Randomly assign a specie level to each observation 
  permuted_species <- with(sub_data, sample(Species, replace = F))

  # --- Difference of means of both species under h0
  p_h0 <- tapply(X = sub_data$Sepal.Length, INDEX = permuted_species, FUN = mean)["virginica"] - 
  tapply(X = sub_data$Sepal.Length, INDEX = permuted_species, FUN = mean)["versicolor"]
  
  # --- Add to null distribution
  null_distribution <- c(null_distribution, p_h0)
}

# CALCULATION OF THE OBSERVED STATISTIC
p_obs <- with(iris, mean(Sepal.Length[Species == "virginica"]) - mean(Sepal.Length[Species == "versicolor"]))
paste0("P_obs = ", p_obs)
```

> Note : here we use `replace = F` in the `sample` function to keep the same number of replicates n each levels of species.

So here we have a $P_{obs}$ value of 0.652. We can now plot the null distribution we generated with the permuted data and place visually $P_{obs}$.

```{r}
# VISUALISATION
hist(null_distribution,
     xlim = c(min(null_distribution) - 0.15, 1),
     breaks = 20,
     main = "",
     xlab = "Statistic value",
     ylab = "Frequency",
     col = rgb(.5, .5, .5, alpha = .2)
     )

abline(v = p_obs, lwd = 1.5, col = "blue", lty = 2)
text(x = .55, y = 1000, labels = TeX("$P_{obs}$"), col = "blue")
```

As expected by the central limit theorem, the distribution of the statistic we use and repeated 10000 times follow a normal distribution. We can clearly see that our statistic calculated on the data $P_{obs}$ stands out of the null distribution. Considering we didn't suppose any direction of the difference, we can determine the p-value as the absolute value of the number of permuted observations $P_{H0}$ greater than the absolute value of $P_{obs}$ divided by the number of iteration :

$$
\text{p-value} = \frac{\text{Nombre de } P_{H0} > P_{obs}}{\text{Nombre d'itération}}
$$

```{r}
sum(abs(null_distribution) > abs(p_obs)) / iteration
```

Here, the p-value of the test is 0.0041. We conclude the difference between both means is significant.

#### Comparison of correlation

```{r}
orange <- Orange

ggplot(data = orange, mapping = aes(x = age, y = circumference, col = Tree)) +
  geom_point() +
  geom_line() +
  theme_classic()

```

## Conclusion on permutation tests
