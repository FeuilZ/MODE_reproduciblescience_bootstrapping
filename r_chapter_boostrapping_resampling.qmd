---
title: "Chapter : Bootstrapping and Resampling"

bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

# Packages

```{r, result = F, message = F, comment=F}
library(tidyverse)
library(latex2exp)
library(datasets)
```

# General introduction

A dataframe:

```{r}
data <- iris
head(iris)
```

## Confidence interval on a sample

Peut-on estimer la taille moyenne

```{r}

hist(data$Sepal.Width)
shapiro.test(data$Sepal.Width)
qqnorm(data$Sepal.Width)
qqline(data$Sepal.Width)
#Or iris dataset ? 
```

L'échantillon est normalement distribué, on peut donc calculer l'intervalle de confiance de la moyenne de la manière suivante:

```{r}
low <- mean(data$Sepal.Width)-1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
high <- mean(data$Sepal.Width)+1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
print(c(low,high))
```

rq: source INSEE taille moyenne F = 1 m 62, et https://ourworldindata.org/grapher/average-height-by-year-of-birth?time=latest&country=\~FRA taille moyenne = 1m6488 pour les gens nés en 1996 (pas de date après) Not in intervall, pop grow, + ech non représentatif de la pop française

## Confidence interval on multiple sub-samples means

Si on veut désormais estimer $\mu$, la taille moyenne des étudiants sur les trois promotions du jeu de données.

```{r}
(mu <- mean(data$Sepal.Width))
```

mu correspond à la taille moyenne qu'on veut estimer mais dont on ne connait pas la véritable valeur.

# The Jackknife resampling method

Imaginons mantenant que nous ayons seulement accès à un échantillon et qu'on veuille estimer la taille moyenne des étudiants de ces trois promotions.

```{r}
set.seed(121)
data_sample <- data[sample(1:150,30),]
mean(data_sample$Sepal.Width)
hist(data_sample$Sepal.Width)
shapiro.test(data_sample$Sepal.Width)
qqnorm(data_sample$Sepal.Width)
qqline(data_sample$Sepal.Width)
```

On a seulement accès à cet échantillon non normalement distribué. La dessus, nous ne pouvons pas calculer les intervalles de confiance comme vu précedemment. C'est là que je Jackknife devient intéressant.

## a - Principe

## b - exemple d'implémentation

test pas du tout fini

```{r}
group <- c()
sampled_data <- c()
for (i in 1:20) { #au total, 100 individus auront été échantillonnés
  set.seed(i)
  sample_temp <- sample(setdiff(1:139,sampled_data),5)
  sampled_data <- c(sampled_data,sample_temp)
  group <- c(group,mean(data[sample_temp,"Sepal.Width"]))
  
}
```

```{r}
hist(group)
qqnorm(group)
qqline(group)
shapiro.test(group)
```

## c - Avantages et inconvénients de la méthode

# Permutation Tests

Permutation tests represent a powerful statistical method for comparing groups and assessing the influence of one variable on another. They aimed to produce statistical inferences and a p-value. In contrast to parametric tests, permutation tests do not rely on specific assumptions about the data distribution, making them more robust in certain situations especially when the number of replicates is low. Furthermore, even if the data in our sample does not adhere to a classical distribution (e.g., Gaussian), statistical inference remains feasible. These tests operate intuitively, are easy to implement, and their robustness makes them particularly practical in numerous scenarios.

## Foundations of Permutation Tests

### Fundamental Principle

Similar to other classical statistical tests (e.g., Student's t-test, $\chi^2$ test), permutation tests are grounded in the distribution of a test statistic under the null hypothesis. The distribution under the null hypothesis corresponds to the theoretical distribution of the test statistic, assuming that the tested effect has no impact (i.e., explanatory variables have no effect on the response variable). In classical statistical tests, this distribution aligns with various known laws, with the most recognized being the Gaussian distribution, although it could also be the Student's t-distribution, the $\chi^2$ distribution, etc. The distinctiveness of permutation tests lies in empirically generating this theoretical distribution directly from the data, putting them in the category of the **exact tests** et **non-parametrics tests**.

According to the central limit theorem, repeating a large number of observations of a random variable (here, the test statistic) leads to a Gaussian distribution. This distribution allows us to position the observed test statistic (calculated on our sample) and determine the significance of the effect of our explanatory variables. In other words, we generate a large number of permutations of the observations, calculate the test statistic for each permutation, and then compare the observed statistic to this null distribution.

### **Parametric tests** : test statistic, null distribution and p-value

The test statistic is a numerical measure used to evaluate whether observed differences between groups in a study are statistically significant. It quantifies the gap between observed and randomly attributable observations. Here are some examples of well-known test statistics:

$$
\begin{align*}
F [\text{ANOVA}] &: \quad F = \frac{MS_{\text{between}}}{MS_{\text{within}}} \quad \sim \mathcal{F}(df_{\text{between}}, df_{\text{within}}) \\
\\
t [\text{Student's t-test}] &: \quad t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \quad \sim t(df) \\
\\
\chi^2 [\text{$\chi^2$ Test}] &: \quad \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i} \quad \sim \chi^2(df) \\
\end{align*}
$$

With $MS$ as the mean square error, $df$ as the degrees of freedom, $\bar{X}_i$ as the mean of group $i$, $s^2_i$ as the variance of group $i$, $n_i$ as the number of replicates in group $i$, $O_i$ as the observed $\chi^2$ distance, and $E_i$ as the expected $\chi^2$ distance.

These statistics usually follow known distributional laws, allowing us to compare the observed statistic (calculated on the data sample) to the null distribution of that statistic and obtain a p-value by comparing the expected values of the statistic under the null hypothesis with the observed statistic obtained with our data. To illustrate, let's say we want to compare the sepal length between the species *I. virginica* and *I. versicolor* using a Student test. First, we plot the data :

```{r, echo = F, fig.height=4, fig.width=4}

boxplot(
  iris$Sepal.Length[iris$Species == "versicolor"],
  iris$Sepal.Length[iris$Species == "virginica"],
  col = c(rgb(1, 0, 0, alpha = .5), rgb(0, 0, 1, alpha = .5)),
  ylab = "Sepal Length",
  xlab = "",
  main = "",
  names = c("I. versicolor", "I. virginica"),
  ylim = c(4, 8)
)

```

*I. virginica* seems to have slightly bigger sepal length than *I. versicolor*. Assuming all conditions are met, we perform a Student test to determine the significance of the difference observed :

```{r}
t.test(
  x = iris$Sepal.Length[iris$Species == "virginica"],
  y = iris$Sepal.Length[iris$Species == "versicolor"],
  paired = F,
  var.equal = T,
  alternative = "two.sided"
)

```

Here we observe a significance difference between the species *I. virginica* and *I. versicolor* with *I. virigina* having bigger sepal length than *I. versicolor*.

Let's get more precise on how do we get the p-value associated with this comparison. The Student distribution under null hypothesis is defined according the degree of freedom, here equal to $df = 98$. Let's plot this $H0$ distribution density of our test :

```{r, fig.height=4, fig.width=4}
# Density distribution of the t statistic
curve(dt(x = x, df = 98), xlim = c(-6, 6), ylab = "Density", xlab = "t")
```

This density distribution represents the probability associated for each value of the $t$ statistic for a degree of freedom of $98$. Now, we have to place in this distribution the $t_{obs}$ calculated from our data, here $t_{obs} = 5.6$. Knowing we have performed a two-sided test (i.e. no *a priori* of the direction of the effect, i.e. the sepal length could have been bigger either for one species or the other), we'll place visually the values of $t$ for which less than $2.5\%$ and more than $97.5\%$ of values are respectively below and above these thresholds. If our $t_{observed}$ falls below the $2.5\%$ or above $97.5\%$ threshold, the difference observed is considered statistically significant.

```{r, fig.height=3.8, fig.width=4}
# Density distribution of the t statistic
curve(dt(x = x, df = 98), xlim = c(-6, 6), ylab = "Density", xlab = "t", lwd = 1.5)

# Quantile function : values of t associated of a two sided test
threshold <- qt(p = c(0.025, 0.975), df = 98)
abline(v = c(threshold[1], threshold[2]), lwd = 1.5, lty = 2, col = "red")


# Color the area of significance
x <- seq(-6, 6, length.out = 100000)
y <- dt(x, df = 98)
polygon(c(x[x<=threshold[1]], threshold[1]), c(y[x<=threshold[1]], y[x == max(x)]), col = "red")
polygon(c(x[x>=threshold[2]], threshold[2]), c(y[x>=threshold[2]], y[x == max(x)]), col = "red")
text(x = 3.7, y = 0.3, labels = TeX("$quantile(1 - \\frac{alpha}{2})$"), col = "red", cex = 0.6)
text(x = -3.5, y = 0.3, labels = TeX("$quantile(\\frac{alpha}{2})$"), col = "red", cex = 0.6)

# Placing the t_obs value
abline(v = 5.6, lwd = 1.5, col = "blue")
text(x = 4.5, y = 0.2, labels = TeX("$t_{obs}$"), col = "blue")
```

With $\alpha$ the type-I error (here 5%). Hence $t_{obs}$ is in the significance area (in red) meaning the difference is significant.

### Permutation test : test Statistic, null distribution and p-value

The method for inference in permutation test is very similar in conception. In permutation test, the statistic test $P_{obs}$ have to be defined. For a test of the difference in means between two groups, for example, the test statistic could be defined as follows:

$$
P = \bar{X}_1 - \bar{X}_2
$$

where ($\bar{X}_1$) and ($\bar{X}_2$) are the means of the two groups compared. But where do we place this $P_{obs}$ value if we don't have a pre-set theoretical distribution like before with the Student distribution? We have to construct the null distribution. To do so, we will permute randomly each value of both group in one group of each other and calculate the test statistic $P_{H0}$ for this new data set. In other word, each data could randomly be assigned either in group 1 or group 2, characterizing the absence of effect of belonging to one group or the other for the response variable as data has been shuffled randomly among both groups. Note that the statistic calculated always has to be independent across the permutation iteration, here it is not really a problem but we'll see later that paired data might constraint the way we shuffle our data. We'll do this procedure a great amount of time and each time extract the $P_{H0}$ obtained. We'll then get a distribution of the test statistic under $H0$ with all the $P_{H0}$ obtained during the iterations. According to the central limit theorem stating the distribution of random variables repeated a large number of time will end up following a bell curve distribution, no matter what shape the original distribution of those variable have. Once we obtained this null distribution, we'll simply follow the same procedure as describe above for the parametric tests and compare our $P_{obs}$ to this null distribution and determine its probability. In permutation test, the p-value corresponds to the number of iteration of $P_{H0}$ above or below (depending of the direction of our hypothesis) the $P_{obs}$ value.

## Implementation of Permutation Tests

### Test Steps

Typical steps to implement a permutation test include:

1.  Formulate null and alternative hypotheses

2.  Determine and calculate the test statistic from the observed data

3.  Generate a large amount of permutations on independent unit of the data and calculate the test statistic for each permutation

4.  Compare the observed statistic to the null distribution of test statistics

5.  Conclude on accepting or rejecting the null hypothesis

### Practical examples

Here we'll present an example of permutation test on independent data and paired data.

#### Independent data (e.g. comparison of means)

Consider an example where we want to test if the difference in means between two groups is statistically significant. We'll use the statistic for means described above and use the same question asked for the Student test example : "Is the sepal length of the genra *I. virginica* and *I. versicolor* different ?" but only with 20 random chosen observations to illustrate how these tests can have a great statistical power with very low replicates unless parametric tests.

```{r}
set.seed(123)

# SELECT THE VARIABLES OF INTEREST
filter_data <- iris %>% 
  filter(Species == "virginica" | Species == "versicolor") %>% 
  droplevels()

# TAKE A SUBSET
sub_data <- filter_data[sample(1:nrow(filter_data), size = 20),]
```

Let's plot the sub-dataset.

```{r, echo = F, out.width="50%", fig.show='hold', , fig.height=4, fig.width=4}
hist(sub_data$Sepal.Length[sub_data$Species == "versicolor"], col = rgb(1, 0, 0, alpha = .5), ylab = "Frequency", xlab = "Sepal Length", main = "Versicolor")
hist(sub_data$Sepal.Length[sub_data$Species == "virginica"], col = rgb(0, 0, 1, alpha = .5), ylab = "Frequency", xlab = "Sepal Length", main = "Virginica", ylim = c(0, 5))

```

Here we can clearly see that the data does not follow a Gaussian distribution like we could have expect with such a few data. Furthermore, neither of the test for comparison of means show a significant differences between sepal length of the two species neither with a Student test or the Mann-Whitney non-parametric test (note that we used a Student test here even if the normal condition is not met. This serves as illustration and comparison with previous test with the full dataset).

```{r, warnings = F}
# T TEST
t.test(
  x = sub_data$Sepal.Length[sub_data$Species == "virginica"],
  y = sub_data$Sepal.Length[sub_data$Species == "versicolor"],
  paired = F,
  var.equal = T,
  alternative = "two.sided"
)

# MANN-WHITNEY TEST
wilcox.test(sub_data$Sepal.Length[sub_data$Species == "virginica"],
  sub_data$Sepal.Length[sub_data$Species == "versicolor"],
  paired = F)
```

This is where permutation test becomes interesting. They have a great power even when the number of replicates is low. Let's see how we proceed.

Permuted data can be generated (among many other ways) by randomly assign levels of the variable 'species' to sepal length values using the `sample` function. Then, the difference of means will be computed to calculate `p_h0` the statistic for the permuted data $P_{H0}$ (the difference of means of sepal length of the permuted data). We'll do this for $i$ `iterations` yielding $i$ number of $P_{H0}$ resulting in the `null_distribution`.

```{r}
# INITIAL PARAMETERS
iteration <- 10000  # number of iteration
null_distribution <- c()  # storage of the statistic for permuted data


# ALGORITHM
for (i in 1:iteration)
{
  # --- Randomly assign a specie level to each observation 
  permuted_species <- with(sub_data, sample(Species, replace = F))

  # --- Difference of means of both species under h0
  p_h0 <- tapply(X = sub_data$Sepal.Length, INDEX = permuted_species, FUN = mean)["virginica"] - 
  tapply(X = sub_data$Sepal.Length, INDEX = permuted_species, FUN = mean)["versicolor"]
  
  # --- Add to null distribution
  null_distribution[i] <- p_h0
}

# CALCULATION OF THE OBSERVED STATISTIC
p_obs <- with(iris, mean(Sepal.Length[Species == "virginica"]) - mean(Sepal.Length[Species == "versicolor"]))
paste0("P_obs = ", p_obs)
```

> Note : here we use `replace = F` in the `sample` function to keep the same number of replicates in each levels of species.

So here we have a $P_{obs}$ value of 0.652. We can now plot the null distribution we generated with the permuted data and compare our observed statistic $P_{obs}$ with the null distribution. Let's plot it to have a visual support on how permutation test works.

```{r}
# CALCULATE SIGNIFICANCE THRESHOLD
bornes <- quantile(null_distribution,c(.025,.975))

# VISUALISATION
hist(null_distribution,
     xlim = c(min(null_distribution) - 0.15, 1),
     breaks = 20,
     main = "",
     xlab = TeX("$P_{H0}$"),
     ylab = "Frequency",
     col = rgb(.5, .5, .5, alpha = .2)
     )

    # POSITION THRESHOLD 
    abline(v = bornes, col = "red", lwd = 2)
    text(x = 0.3, y = 1400, labels = TeX("$quantile(1 - \\frac{alpha}{2})$"), col = "red", cex = 0.8)
    text(x = -0.65, y = 1400, labels = TeX("$quantile(\\frac{alpha}{2})$"), col = "red", cex = 0.8)

    # POSITION P_OBS
    abline(v = p_obs, lwd = 2, col = "blue", lty = 2)
    text(x = .75, y = 1400, labels = TeX("$P_{obs}$"), col = "blue")
```

As expected by the central limit theorem, the distribution of the statistic we use and repeated 10000 times follow a normal distribution. We can clearly see that our statistic calculated on the data $P_{obs}$ stands out of the null distribution. Considering we didn't suppose any direction of the difference, we can determine the p-value as the absolute value of the number of permuted observations $P_{H0}$ greater than the absolute value of $P_{obs}$ divided by the number of iteration :

$$
\text{p-value} = \frac{\text{Nombre de } P_{H0} > P_{obs}}{\text{Nombre d'itération}}
$$

```{r}
sum(abs(null_distribution) > abs(p_obs)) / iteration
```

Here, the p-value of the test is 0.0041. We conclude the difference between both means is significant.

#### Paired data (e.g. comparison of correlation)

In this part, we will illustrate how to perform permutation test on paire data. We'll use de `iris` dataset again a imagine a scenario where a fertilizer has been applied to each individual plant. The question will still concern the sepal length and we will explore if the fertilizer induces sepal length growing in a before / after experimental paradigm. As a consequence, the data are paired by individuals. As each unit of permutation has to be independent, otherwise the test is not valid, here we have to constraint how our values will be permuted by individuals. In contrast to previous permuted procedure where all data were merely shuffled together regardless of their belonging to one species, here we have to permute the data **within** individual to account for the non-independence of the measures within each individual.

To simply the procedure, we will only consider the species *I. versicolor*. Let's create the sub data and generate the new variable.

```{r}
# FILTERING DATASET FOR VERSICOLOR
data_versicolor <-
  data %>%
  filter(Species == "versicolor") %>%
  droplevels()

# GENERATION OF THE NEW VARIABLE 
# --- Generation of the new variable Sepal.Length.T2 drawn drom normal distribution
Sepal.Length.T2 <- round(
  runif(nrow(data_versicolor), min = .8, max = 1.2) * data_versicolor$Sepal.Length,
  digits = 1
)
  
# --- Adding the new variable to the dataset
data_versicolor <- 
  data_versicolor %>% 
  mutate(Sepal.Length.T1 = Sepal.Length,
         Sepal.Length.T2 = Sepal.Length.T2)

```

The new created variable `Sepal.Length.T2` follow a Gaussian distribution of mean equal to the mean of the sepal length at T1 + 0.1 and of standard deviation 3 :

$$
Sepal.Length.T2 \sim \mathcal{N}(\bar{x} = Sepal.Length.T1) + 0.1, \sigma = 3)
$$

Here we'll test using permutation test using the precedent statistic for difference in mean. Let's plot this relation :

```{r}
boxplot(
  data_versicolor$Sepal.Length.T1,
  data_versicolor$Sepal.Length.T2,
  col = c(rgb(1, 0, 0, alpha = .5), rgb(0, 0, 1, alpha = .5)),
  ylab = "Sepal Length",
  xlab = "",
  main = "",
  names = c("T1", "T2"),
  ylim = c(3, 9)
)

```

According to the visualization, the effect of the fertilizer does not seem to have a great effect on the sepal length.

Let's follow step by step the permutation procedure :

1.  **Formulate null and alternative hypotheses**

    **H1** : the fertilizer has a positive effect on the sepal length, sepal measure at T2 is expected to be greater than T1 **H0** : the fertilizer has no effect, no differences between T1 and T2

2.  **Determine and calculate the test statistic from the observed data**

    We'll use the coefficient used precedently : P = \bar{X}*{T1} -* \bar{X}{T2}. We calculate the test statistic for $P_{obs}$ :

    ```{r}
    p_obs <- mean(data_versicolor$Sepal.Length.T1) - mean(data_versicolor$Sepal.Length.T2) 
    p_obs
    ```

3.  **Generate a large amount of permutations on independent unit of the data and calculate the test statistic for each permutation**

    Considering the paired status of the data, we will shuffle randomly the values of the variable `Sepal Length` **within** each individual to construct the null hypothesis. So rather than permuting data in the whole column like before, we will permute by row to constrain the permutation by individual.

    ```{r}
    # INITIAL PARAMETERS
    iteration <- 10000  # number of iteration
    null_distribution <- c()  # storage of the statistic for permuted data


    # ALGORITHM
    for (i in 1:iteration)
    {
      # --- Randomly assign age data witihin individual modality
      permuted_data <-
        t(apply(
          X = data_versicolor[c("Sepal.Length.T1", "Sepal.Length.T2")],
          MARGIN = 1,
          FUN = sample,  # randomly shuffle the two values of each rows
          simplify = T   # to have a matrix output (and not a list)
        ))

      # --- Difference of means of both species under h0
      p_h0 <- mean(permuted_data[,1]) - mean(permuted_data[,2])
      
      # --- Add to null distribution
      null_distribution <- c(null_distribution, p_h0)
    }
    ```

4.  **Compare the observed statistic to the null distribution of test statistics**

    ```{r}
    # CALCULATE SIGNIFICANCE THRESHOLD
    bornes <- quantile(null_distribution,c(.95))

    # VISUALISATION
    hist(null_distribution,
         breaks = 30,
         main = "",
         xlab = TeX("$P_{H0}$"),
         ylab = "Frequency",
         col = rgb(1/3, 1/3, 1/3, alpha = .1)
         )

    # POSITION THRESHOLD 
    abline(v = bornes, col = "red", lwd = 2)
    text(x = .24, y = 800, labels = TeX("$quantile(1 - \\alpha)$"), col = "red")

    # POSITION P_OBS
    abline(v = p_obs, lwd = 2, col = "blue", lty = 2)
    text(x = -.08, y = 860, labels = TeX("$P_{obs}$"), col = "blue")
    ```

    We calculate the p-value :

    ```{r}
    sum(abs(null_distribution) > abs(p_obs)) / iteration
    ```

5.  **Conclude on accepting or rejecting the null hypothesis**

    As $p > 0.05$, we don't reject the null hypothesis stating the a difference of means would be observed before and after fertilizer usage. Note here that we only used null statistic which were greater than the observed statistic for the calculation of the p-value because the hypothesis was one-sided (we made an asumption of direction by expecting a positive difference).

### Permutation test on bootstrapped data

Exploring the precision of our calculated statistic becomes particularly insightful. Taking the case of the study on the difference in sepal length between *I. virginica* and *I. versicolor* as an example, we are interested in determining the level of accuracy associated with the observed mean difference. To achieve a robust assessment, we opt for a combined approach, integrating both bootstrap and permutation methodologies. In this process, we initiate a permutation test on each pseudo-data set generated through bootstrapping. The objective is to collect the statistic for each iteration, resulting in a distribution that effectively communicates the variance of estimation linked to the difference in means. This combined technique not only provides a confidence interval around our statistic but also enhances the overall reliability of our findings by accounting for the complexities and uncertainties inherent in the data. Note that combining both re-sampling techniques can be very costly in terms of calculation time.

## Conclusion on permutation tests

In conclusion, permutation tests offer a powerful and flexible approach to statistical inference, particularly when traditional parametric assumptions cannot be met. By iteratively permuting the observed data, these tests generate a null distribution under the assumption of no effect, allowing for the assessment of the observed statistic's significance. This makes permutation tests applicable in various scenarios, including comparisons of means, medians, correlations, or any other measure of interest.

Although permutation tests can be costly in terms of computation (especially when paired to bootstrap procedure), the key advantage of permutation tests lies in their ability to provide reliable results with small sample sizes and non-normally distributed data. Unlike parametric tests, permutation tests do not rely on assumptions about the underlying distribution of the data, making them robust and applicable in a wide range of situations.

In the next chapter, we delve into the Mantel test, a statistical method used to assess the correlation between two distance matrices. This test is particularly valuable in fields such as ecology and genetics, where understanding the spatial or temporal relationships between entities is crucial.
