------------------------------------------------------------------------

\-\-- title: "Chapter : Bootstrapping and Resampling"

bibliography: references.bib execute: freeze: auto output: html_document: toc: true toc_float: true \-\--

# General introduction

A dataframe:

```{r}
data <- iris
head(iris)
```

## Confidence interval on a sample

Peut-on estimer la taille moyenne

```{r}

hist(data$Sepal.Width)
shapiro.test(data$Sepal.Width)
qqnorm(data$Sepal.Width)
qqline(data$Sepal.Width)
#Or iris dataset ? 
```

L'échantillon est normalement distribué, on peut donc calculer l'intervalle de confiance de la moyenne de la manière suivante:

```{r}
low <- mean(data$Sepal.Width)-1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
high <- mean(data$Sepal.Width)+1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
print(c(low,high))
```

rq: source INSEE taille moyenne F = 1 m 62, et https://ourworldindata.org/grapher/average-height-by-year-of-birth?time=latest&country=\~FRA taille moyenne = 1m6488 pour les gens nés en 1996 (pas de date après) Not in intervall, pop grow, + ech non représentatif de la pop française

## Confidence interval on multiple sub-samples means

Si on veut désormais estimer $\mu$, la taille moyenne des étudiants sur les trois promotions du jeu de données.

```{r}
(mu <- mean(data$Sepal.Width))
```

mu correspond à la taille moyenne qu'on veut estimer mais dont on ne connait pas la véritable valeur.

# The Jackknife resampling method

Imaginons mantenant que nous ayons seulement accès à un échantillon et qu'on veuille estimer la taille moyenne des étudiants de ces trois promotions.

```{r}
set.seed(121)
data_sample <- data[sample(1:150,30),]
mean(data_sample$Sepal.Width)
hist(data_sample$Sepal.Width)
shapiro.test(data_sample$Sepal.Width)
qqnorm(data_sample$Sepal.Width)
qqline(data_sample$Sepal.Width)
```

On a seulement accès à cet échantillon non normalement distribué. La dessus, nous ne pouvons pas calculer les intervalles de confiance comme vu précedemment. C'est là que je Jackknife devient intéressant.

...

## a - Principe

## b - exemple d'implémentation

test pas du tout fini

```{r}
group <- c()
sampled_data <- c()
for (i in 1:20) { #au total, 100 individus auront été échantillonnés
  set.seed(i)
  sample_temp <- sample(setdiff(1:139,sampled_data),5)
  sampled_data <- c(sampled_data,sample_temp)
  group <- c(group,mean(data[sample_temp,"Sepal.Width"]))
  
}
```

```{r}
hist(group)
qqnorm(group)
qqline(group)
shapiro.test(group)
```

## c - Avantages et inconvénients de la méthode

# The Bootstrap re-sampling method

## Introduction and Principle

Now that we've extensively covered the Jackknife resampling in the previous section, let's delve into another resampling technique: the Bootstrap method.

Similar to Jackknife, Bootstrap aims to estimate descriptive parameters of a sample and evaluate the accuracy of these estimates to make inferences about the actual population. It is yet another method of statistical inference through multiple replications of data based on the initial dataset. Basically, the idea is to use resampling to create a probability distribution of any chosen parameter. This distribution probability, know as the empirical distribution function, will be an estimation of what really is the distribution probability of our parameter in the real population. In practice, akin to the previous section, our goal here is to derive parameters of interest from our sample (such as mean, median, or even the $R^2$ of a regression, among others) and construct confidence intervals around these estimates.

The key distinction between Jackknife and Bootstrap lies in how, in each of its $i$ iterations, Bootstrap resamples $n$ elements with replacement from an initial sample of $n$ data. Put differently, during each iteration, elements are drawn from the initial sample and then put back in after each random selection. This implies two important differences from the resampling we achieved using Jackknife: i) the new samples maintain the same size $n$ as the original dataset, ii) a specific element may appear multiple times in a new sample or may not appear at all.

Once these $i$ Bootstrap samples are created, the desired statistical parameters are calculated for each of these samples. Consequently, we obtain a distribution consisting of $i$ data points from these samples, this is our empirical distribution function. Finally, it is by analyzing this distribution that we can estimate precision, particularly the confidence interval for our statistical parameters.

## Practice and Applications

A bit confused about this previous introduction? No worries, we'll take an example to illustrate this.

For this part, we will work again on the Iris RBase dataset. Let's say that we are interested in the average width of the sepals in a wild population of Iris flowers. As we couldn't measure the sepals of every single flower, we sampled and measured a given number of individuals on field. To illustrate this, we'll consider our Iris dataset to be our complete wild population (which of course, you'll never have access to in real condition), and take only a few individuals to represent our field sampling.

```{r}
# Our real population
irisPopulation <- iris$Sepal.Width
# Our field sampling
set.seed(42)
irisSampling <- irisPopulation[sample(1:150,15)]
```

From this field sampling, we are easily able to compute a mean, which we'll be doing:

```{r}
# Field sampling mean
mean(irisSampling)
```

So, we got a mean value of $3.15$, great. However, this mean can hardly be extrapolated to the actual population, as we don't have any information of the variability induced by the individuals we decided to sample. Especially, you can see in our previous code, that we only sampled $n=15$ individuals on field. This is clearly not enough sampling to assume that our estimated mean is representative of our actual population. So, we might want to have some information about the variability of the mean we just estimated, and this is where Bootstrap method joins the game.

We are now going to apply step by step our algorithm to resample our initial field sampling dataset, and use to build the probability distribution of our mean.

1.  **Make our Bootstrap resampling iterations**

```{r}
# Fix our number of bootstrap iterations
B = 1000

# Create an empty list to stock our resamplings
bootstrapSamples <- vector("list",B)

# Bootstrap resampling algorithm
for (i in 1:B) {
  # Randomly sample n elements with replacement
  bootstrapSamples[[i]] <- sample(irisSampling, replace = TRUE)
}
```

For pedagogical purpose, let's give a look at a few resamples while comparing them to our initial field sampling dataset.

```{r}
# Our original field sampling
sort(irisSampling)
```

```{r}
# A few resampling
sort(bootstrapSamples[[5]])
sort(bootstrapSamples[[777]])
```

See how some values are repeated more times than in our initial dataset? And that some others aren't sampled? For instance, $2.5$ appeared only one time in our initial set and was therefore rarely or never resampled. Meanwhile, $2.8$ was more common, and end up being really often sampled.

2.  **Compute mean for every resampled datasets**

Now we are going to compute our parameter of interest of every of our resampled datasets. This will give us our set of $i=1000$ mean values, from which we will create our empirical distribution in the next step.

```{r}
# Compute mean for each boostrap samples
iterationMeans <- sapply(bootstrapSamples, mean)
```

3.  **Plot the probability distribution**

The distribution of our parameter of interest is now done, we can therefore plot it to have an idea of the variability of our estimated mean value.

```{r}
# Probability distribution plot
ggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +
  geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black", aes(y =after_stat(density))) +
  geom_density(alpha = 0.5, fill = "orange") +
  geom_vline(xintercept = mean(irisSampling), color = "red", linetype = "dashed") +
  labs(title = "Empirical probability distribution of mean values",
       x = "Mean",
       y = "Density") +
  theme_minimal()
```

This probability distribution therefore give us information about the variability around the mean value estimated with our field sampling. As expected and as represented with the red dashed line, this distribution is centered around this estimated mean value.

4.  **Compute confidence interval around our mean estimate**

Remember the original purpose of the Bootstrap method? Evaluate the accuracy around the estimation of our parameter to make inferences to our natural population. And to make so, we are going to use the Bootstrap percentiles. Let's say we want to compute the 95% Confidence Interval. To do so, we will split the 5% of error between each side of our distribution and therefore took the values corresponding to 2.5 and 97.5% in our empirical distribution.

```{r}
# Get percentile 2.5%
quantile(iterationMeans, probs = 0.025)
```

```{r}
# Get percentile 97.5%
quantile(iterationMeans, probs = 0.975)
```

And this is how we finally got our confidence interval for our mean estimation : $2.97 \leq \overline{x} \leq 3.33$.

Once again for pedagogical purpose, we can plot those percentiles.

```{r}
# Probability distribution plot
ggplot(data = data.frame(iterationMeans), aes(x = iterationMeans)) +
  geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black", aes(y =after_stat(density))) +
  geom_density(alpha = 0.5, fill = "orange") +
  geom_vline(xintercept = mean(irisSampling), color = "red", linetype = "dashed") +
  geom_vline(xintercept = quantile(iterationMeans, probs = 0.025), color = "green", linetype = "dashed") +
  geom_vline(xintercept = quantile(iterationMeans, probs = 0.975), color = "green", linetype = "dashed") +
  labs(title = "Empirical probability distribution of mean values",
       x = "Mean",
       y = "Density") +
  theme_minimal()
```

5.  \*\*Compare with what

**Calcule de l'intervalle de confiance autour de la moyenne sous hypothèse de normalité :** Nous allons pour le moment considérer le jeu de données entier et calculer l'intervalle de confiance autour de sa moyenne.

```{r}
## Calcule de la moyenne de l'échantillon
m_petalW = mean(dataPetal)
cat("Moyenne de l'échantillon :", m_petalW)
```

On obtient donc une moyenne $\overline{x} \approx 1.20$ pour la largeur des pétales. En faisant l'hypothèse, comme vu précédemment pour la méthode de Jacknife, que nos données suivent la loi normale, nous allons pouvoir calculer l'intervalle de confiance autour de la moyenne d'un échantillon tel que :

$$
IC = \overline{x} \pm t \cdot (\frac{s}{\sqrt{n}})
$$

Avec comme valeurs :

-   $\overline{x}$ : la moyenne de l'échantillon

-   $t$ : le score de t de Student correspondant à l'intervale de confiance choisi et à un degré de liberté $(n-1)$.

-   $s$ : l'écart-type de la l'échantillon.

-   $n$ : la taille de l'échantillon.

```{r}
## Calcule de l'intervalle de confiance 
# Calcul de l'écart type de l'échantillon
et_petalW <- sd(dataPetal)

# Calcul du coefficient de Student pour un niveau de confiance à 95%
coef_t <- qt(0.995, df = length(dataPetal) - 1)

# Calcul de l'intervalle de confiance
IC <- coef_t * (et_petalW / sqrt(length(dataPetal)))
borne_inferieure <- m_petalW - IC
borne_superieure <- m_petalW + IC

# Affichage de l'intervalle de confiance
cat("Valeur de l'intervale de confiance à 95%: ",IC,"\n")
cat("Intervalle de confiance à 95% autour de la moyenne :", borne_inferieure, "-", borne_superieure, "\n")
```

On obtient donc avec cette méthode une moyenne avec intervalle de confiance à 95% de $120 \pm 0.16$.

**Calcule de l'intervalle de confiance avec méthode par Bootstrap :** Comme réalisé précédemment pour la méthode par Jacknife, nous allons utiliser la méthode de rééchantillonnage pour estimer l'intervalle de confiance autour de la moyenne à partir d'une petite portion du jeu de données seulement. Ceci nous servira de cas d'école pour comparer les estimations de la méthode avec le jeu de données original que l'on considérera comme une réalité connue.

```{r}
## Récupération d'une partie seulement du jeu de données
set.seed(42)
data_sample <- dataPetal[sample(1:150,15)]
```

On va donc commencer par réaliser les $i$ itérations du rééchantillonnage. On fixera ici une valeur de $i=1000$ itérations, où pour chacune d'entre elles on tirera aléatoirement avec remise l'ensemble des données et on calculera la moyenne.

```{r}
## Création d'un vecteur qui contiendra les itérations
boots = numeric(1000)

## Boucle des iérations avec calcule de la moyenne
for (i in 1:1000) {
  boots[i] = mean(sample(data_sample, replace=T))
}
```

Maintenant que l'on a $i=1000$ moyennes calculées à partir de nos rééchantillonnages par Bootstrap, on va pouvoir représenter leur distribution.

```{r}
# Création du graphique avec ggplot2
ggplot(data = data.frame(boots), aes(x = boots)) +
  geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black", aes(y = ..density..)) +
  geom_density(alpha = 0.5, fill = "orange") +
  labs(title = "Distribution des valeurs",
       x = "Valeurs",
       y = "Densité") +
  theme_minimal()
```

De cette distribution de densité, on va pouvoir récupérer les percentiles correspondants à notre intervalle de confiance. Dans notre exemple on souhaitera obtenir un intervalle de confiance à 95%, on répartira donc les 5% d'erreur de part et d'autre de la distribution, ce qui nous donne les percentiles à 2.5% et 97.5%.

```{r}
## Obtention des percentiles à 2.5 et 97.5%
limBoot1=quantile(boots,c(.025,.975))
limBoot1
```

L'intervalle de confiance n'est pas parfaitement équivalent, ce qui est normal car le jeu de données sur lequel nous avons travaillé n'est qu'une très petite portion du jeu de données initial. Cependant, on constate qu'à partir de cette méthode, on obtient un intervalle de confiance qui inclue la moyenne du jeu de données de base.

## Strenghts and weaknesses

Un des avantages majeurs de la méthode Bootstrap est que contrairement à la méthode Jacknife elle marche beaucoup mieux pour des échantillons de petite taille.

# Tests de permutation

## Introduction

Les tests de permutations sont une méthode statistique puissante pour comparer des groupes et évaluer l'effet d'une variable sur une autre. Contrairement aux tests paramétriques, les tests de permutations ne reposent pas sur des hypothèses spécifiques concernant la distribution des données, les rendant ainsi plus robustes dans certaines situations. Autrement dit, même si les données de notre échantillon ne suive pas une loi classique (e.g. gaussienne), l'inférence statistique reste possible. Ce sont des tests dont le fonctionnement est intuitifs, facile à mettre en place et leur robustenne les rend particulièrement pratique dans un grand nombre de situations.

## Fondements des Tests de Permutations

### Principe Fondamental

A l'instar des autres tests statistiques classiques (e.g. T de Student, test du $\chi 2$...), les tests de permutations sont basés sur une distribution d'une statistique de test sous l'hypothèse nulle. La distribution sous l'hypothèse nulle correspond à la distribution théorique de la statistique du test en supposant que l'effet que nous testons n'a pas d'impact (i.e. que nos variables explicatives n'ont pas d'effet sur la variable dépendante). Dans les tests statistiques classiques, cette distribution correspond à différentes lois connues, la plus connue étant la loi Gaussienne, mais ça peut-être la distribution de Student, la distribution $\chi 2$ etc. La particularité des tests de permutations est que l'on va générer cette distribution théorique de façon empirique directement à partir des données. Les tests de permutation sont des tests non-paramétriques (ils ne font pas d'approximation de distribution) et exacts (basés sur les données uniquement).

Selon les règles du théorème centrale limite, répéter un grand nombre de fois une observation d'une variable aléatoire (ici la statistique de test) ménera à une distribution Gaussienne dans laquelle on pourra placer la statistique de test observée (i.e. calculée sur notre échantillon) et ainsi définir la significativité de l'effet de nos variables explicatives. En d'autres termes, on génère toutes les permutations possibles des observations, calculons la statistique de test pour chaque permutation, puis comparons la statistique observée à cette distribution nulle.

### Statistique de Test

La statistique d'un test est une mesure numérique utilisée pour évaluer si les différences observées entre les groupes dans une étude sont statistiquement significatives. Elle permet de quantifier l'écart entre les observations observées et celles qui pourraient être attribuées au hasard. Voici quelques exemples de statistiques de tests connues : \$\$ \begin{align*}
F [\text{ANOVA}] &: \quad F = \frac{MS_{\text{intergroupes}}}{MS_{\text{intragroupes}}} \quad \sim \mathcal{F(ddl_{\text{intergroupes}}, ddl_{\text{intragroupes}})} &&\\
\\
t [\text{t de Student}] &: \quad t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \quad \sim t(ddl) &&\\
\\
\chi^2 [\text{Test du } \chi^2] &: \quad \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i} \quad \sim \chi^2(ddl) &&\\
\end{align*}

\$\$ Avec $MS$ l'erreur moyenne carrée, $ddl$ le degré de liberté, $\bar{X}_i$ la moyenne du groupe $i$, $s^2_i$ la variance du groupe $i$, $n_i$ le nombre de réplicats du groupe $i$, $O_i$ la distance $\chi^2$ observée, $E_i$ la distance $\chi^2$ attendue.

Ces statistiques suivent des loi de distribution connues, on va donc comparer la statistique observée (calculé sur l'échantillon de données) à la distribution nulle de cette statistique et obtenir ainsi une p-valeur.

Ces staomme La statistique de test pour les tests de permutations dépend du type d'étude et de la relation entre les données (indépendance ou non-indépendance). Pour un test de différence de moyennes entre deux groupes, par exemple, la statistique de test pourrait être définie comme suit :

\[ T = \frac{\bar{X}_1 - \bar{X}_2}{s_{\text{pooled}}} \sqrt{\frac{n_1 n_2}{n_1 + n_2}}, \]

où (\bar{X}\_1) et (\bar{X}*2) sont les moyennes des deux groupes, (s*{\text{pooled}}) est l'écart-type combiné des groupes, et (n_1) et (n_2) sont les tailles des échantillons.

## Mise en œuvre des Tests de Permutations

### Étapes du Test

Les étapes typiques pour mettre en œuvre un test de permutations comprennent :

1.  Formuler les hypothèses nulle et alternative.
2.  Calculer la statistique de test à partir des données observées.
3.  Générer toutes les permutations possibles des données.
4.  Calculer la statistique de test pour chaque permutation.
5.  Comparer la statistique observée à la distribution nulle des statistiques de test.
6.  Conclure sur l'acceptation ou le rejet de l'hypothèse nulle.

## Exemples Pratiques

### Test de Permutations pour la Différence de Moyennes

Considérons un exemple où nous voulons tester si la différence de moyennes entre deux groupes est statistiquement significative. La statistique de test est définie comme dans l'équation ci-dessus.

### Test de Permutations pour la Corrélation

Dans le cas d'une corrélation, la statistique de test pourrait être basée sur la mesure de corrélation de Spearman. Les étapes décrites précedemment devront être suivi également.
