---
title: "Chapter : Bootstrapping and Resampling"

bibliography: references.bib
execute: 
  freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

# Introduction

? changer le titre en : Permutation tests and Resampling

classical statistics : useful to calculate p-values & confidence intervals (recoupe l'introduction de PA, est-ce qu'elle ne serait pas utile ici ? )
problem when not enough data and/or does not follow known distribution

-> non parametric tests = tests which does not require a particular distribution of the variables
resampling : 
https://www.sciencedirect.com/topics/mathematics/resampling-method
    - jackknife
    - bootstrap

permutations
https://www.sciencedirect.com/topics/mathematics/permutation-test
    - mantel (multivariate)


To illustrate the following methods, lets use a dataset:

```{r}
data <- iris
head(iris)
```

## Confidence interval on a sample

```{r}
library(ggplot2)

ggplot(data)+
  geom_histogram(aes(x = Sepal.Width),colour = "black", fill = "white", bins = 12)+
  xlab("Sepal width")+
  labs(title = " Sepal width distribution in the dataset") +
  theme_classic()

shapiro.test(data$Sepal.Width)

ggplot(data)+
  stat_qq(aes(sample=Sepal.Width))+
  stat_qq_line(aes(sample=Sepal.Width), color="red")+
  xlab("Normal theoretical quantiles")+
  ylab("Dataset quantiles")+
  labs(title = " Sepal width QQ-plot ") +
  theme_classic()

```

The sample being normally distributed, we can estimate a confidence interval as below:

```{r}
low <- mean(data$Sepal.Width)-1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
high <- mean(data$Sepal.Width)+1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))
print(c(low,high))
```

## Confidence interval on a sub-sample

Let's imagine now that we only have access to a smaller data set. The full iris dataset is the population from which we want to estimate a statistical parameter for instance the mean.

```{r}
mu <- mean(data$Sepal.Width)
print(mu)
```

Here $\mu$ is the "real" mean of the population that theoretically we do not know.\
We only have access to the following sub-sample of 15 observations:

```{r}
set.seed(42)
data_sample <- data[sample(1:150,15),]

ggplot(data_sample)+
  geom_histogram(aes(x = Sepal.Width),colour = "black", fill = "white", bins = 12)+
  xlab("Sepal width")+
  labs(title = " Sepal width distribution in the sub-sample ") +
  theme_classic()

ggplot(data_sample)+
  stat_qq(aes(sample=Sepal.Width))+
  stat_qq_line(aes(sample=Sepal.Width), color="red")+
  xlab("Normal theoretical quantiles")+
  ylab("Sub-sample quantiles")+
  labs(title = " Sepal width QQ-plot ") +
  theme_classic()
```

This sample is not normally distributed.

```{r}
mean(data_sample$Sepal.Width)
```

The mean is 3.15, the problem is that we can not estimate if the mean that we observe is far from the true mean. We don't have any method with classical statistics to estimate a confidence interval. This is where resampling methods can become interesting.

# The Jackknife resampling method

This method was first proposed in 1949 by Maurice Quenouille ( **Source Quenouille** ). The name "Jackknife" comes from the fact that it is often referenced as a "quick and dirty" tool of statistics (**Source Abdi)**. It means that it is usable in many situations but it is often not the best tool. The technique allows us to estimate a confidence interval for some statistics when the dataset is too small and/or when it does not follow a known distribution.

## Principle of the leave-one-out Jackknife (source: shinaray & cours eric petit)

The main goal of the jackknife method is to calculate a confidence interval around a statistic when classical statistics can not apply to the data.The principle is to create subsamples of the initial sample by successively removing some of the observations. Then, on each subsample, a pseudo-value will be calculated. With all the pseudo-values, it is possible to calculate an estimator of the statistic and to estimate its confidence interval.

The most famous jackknife is the the leave-one-out jackknife (or order 1 jackknife) for which all the subsamples contain all the observations except one. Concretely, for a sample of $n$ observations, there will be $n$ subsamples of size $(n-1)$. The $i^{th}$ subsample will be composed of observations from $1$ to $n$ minus the $i^{th}$ observation. 

```{r,echo=FALSE, results='hide'}
# A simple representation of all the subsamples is proposed below with the subsamples in lines and the observation numbers in columns.


subsample_representation <- cbind(rep(seq(1,nrow(data_sample)),nrow(data_sample)),
                                  sort(rep(seq(1,nrow(data_sample)),
                                           nrow(data_sample))),
                                  rep(1,nrow(data_sample)*nrow(data_sample)))

colnames(subsample_representation) <- c("subsample", "observation", "value")
subsample_representation <- as.data.frame(subsample_representation)
subsample_representation[subsample_representation$subsample == subsample_representation$observation,"value"] <- 0


ggplot(subsample_representation)+
  geom_tile(aes(x = observation ,y = subsample, fill = as.factor(value)), color = "white")+
  theme_classic()+
  scale_fill_manual(values = c("#CCCCFF","#666699"),name = "", labels = c("Removed of \n the subsample", "Kept in \n the subsample"))+
  ylab("Subsample number")+
  xlab("Observation number")+
  labs(title = "Representation of the subsamples in the leave-one-out \n jackknife method")
```

## Calculation of pseudo-values

After creating the subsamples, the next step is to calculate pseudo-values for each of the new subsamples. The formula for pseudo-values depend on the statistic of interest. In our case we want to estimate the mean, the formula will then be:

$$
v_{i} = n\overline{X} - (n-1)\overline{X}_{-i}
$$

with the following variables:

|      Variable       | Meaning                                                                             |
|:-------------------:|-------------------------------------------------------------------------------------|
|       $v_{i}$       | Pseudo value of the $i^{th}$ subsample                                              |
|         $n$         | Total number of observations                                                         |
|   $\overline{X}$    | Mean of the initial sample                                                          |
| $\overline{X}_{-i}$ | Mean of the $i^{th}$ subsample, corresponding to all observations except the $i{th}$ |

Let's write a function which create the subsamples and calculate their pseudo values:

```{r}
pseudo_val <- function(data, theta){
  #entry : data = the vector of data to which we want to apply the Jackknife
  #entry : theta = function for the statistic of interest
  #output : a vector of pseudo values for each subsample
  n <- length(data)
  mean_init <- theta(data)
  pv <- rep(NA,n) #to keep in memory each pseudo value
  for (i in 1:n) {
    pv[i] <- n*mean_init - (n-1)*theta(data[-i])
  }
  return(pv)
}

```

To try the function:

```{r}
pv <- pseudo_val(data =  data_sample$Sepal.Width,
                  theta = mean)
print(pv)
```

## Test statistic

The jackknife estimator $\theta$ of the mean will then be calculated as follow:

$$
\begin{align}
\hat{\theta} & = \frac{\sum_{i=1}^{n}v_{i}}{n} \\
\hat{\theta} & = \overline{v}
\end{align}
$$

It corresponds to the mean of the pseudo values $v_i$.


```{r}
mean_pv <- mean(pv)
mean_pv
```

The Jackknife estimator $\theta$ obtained is 3.15. This technique supposes that the Jackknife estimator is normally distributed and its standard error is calculated as follow:

$$
\begin{align}
SE_\hat{X} & = \sqrt{\frac{\sum_{i=1}^{n}(v_{i}-\overline{v})}{n(n-1)}} \\
SE_\hat{X} & = \sqrt{\frac{\sigma_{v}^{2}}{n}}
\end{align}
$$

With $\overline{v}$ being the mean of the pseudo values (and the jackknife estimator).

```{r}
SE <- sqrt(var(pv)/length(pv))
SE
```

## Confidence interval

From these we can calculate a confidence interval:

$$
[\, \overline{v} - 1.96 \, SE_\hat{X} ; \overline{v} + 1.96 \, SE_\hat{X} \,]
$$

> Note: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.

```{r, echo=FALSE}
cat(" Lower bound : ",
    as.character(round(x = mean_pv - 1.96*SE,digits = 2)),
    "\n",
    "Higher bound : ",
    as.character(round(x = mean_pv + 1.96*SE,digits = 2)))
```

## R package 

On r, functions already exist to automatically execute the Jackknife. For instance, in the package bootstrap, there is a function jackknife. The function take as entry a vector containing the data (x) and a function indicating which statistic needs to be estimated (theta).

```{r}
library("bootstrap")
print(jackknife(x= data_sample$Sepal.Width,
                theta = function(x) mean(x)))
```

The output of the function include the standard error (`$jack.se`) as describe above. It also include the bias (`$jack.bias`) which is the difference between the initial sample statistic and the jackknife estimated statistic. It is important to note that the output `$jack.values` does not correspond to the pseudo values but to the statistic of interest calculated on every subsample. In our example, it correspond to the mean of each subsample.

## Advantages of the method

As seen precedently, the Jackknife allows to calculate a confidence intervall when data are not following a normal distribution and to estimate the bias induced when only a sample of the statistical population is observed. These two characteristic does not only apply to univariate estimators such as the mean. It can be used on correlation coefficients and regression parameters for instance.  (**Shinaray**). In that case, each subsample leave one observations out with all its associated variables. The jackknife method is peticularly interesting when it comes to make inferences about variance ratio. It can perform as good as the Fisher test if variables are normally distributed and better if they are not. (**Miller**).  
The Jackknife method was a good tool in the last century as it was easy to apply manually but it is nowaday surpassed by other methods which emerged thanks to the evolution of computers.

## Disadvantages of the method

This method is not always efficient, it is not recomanded for time series analysis where some time period are successively removed to create the subsample. Moreover it has little succes when it comes to the estimation of single order statistics (specific values in an ordered set of observations or data points) such as the median or the maximum (**Miller**). But for this last point, other jackknife can be used such as the deleted-d jackknife which perform better for the median. In this method, subsamples are of size $(n-d)$ and there are $\binom{n}{d}$ subsamples.  


For more informations about applications of the Jackknife you can read the following paper https://www.jstor.org/stable/pdf/2334280.pdf?refreqid=fastly-default%3Af728183558d4928f9eddd7a58f8406a9&ab_segments=&origin=&initiator=&acceptTC=1 


# Tests de permutation
## Introduction
Les tests de permutations sont une méthode statistique puissante pour comparer des groupes et évaluer l'effet d'une variable sur une autre. Contrairement aux tests paramétriques, les tests de permutations ne reposent pas sur des hypothèses spécifiques concernant la distribution des données, les rendant ainsi plus robustes dans certaines situations. Autrement dit, même si les données de notre échantillon ne suive pas une loi classique (e.g. gaussienne), l'inférence statistique reste possible. Ce sont des tests dont le fonctionnement est intuitifs, facile à mettre en place et leur robustenne les rend particulièrement pratique dans un grand nombre de situations.


## Fondements des Tests de Permutations
### Principe Fondamental

A l'instar des autres tests statistiques classiques (e.g. T de Student, test du $\chi 2$...), les tests de permutations sont basés sur une distribution d'une statistique de test sous l'hypothèse nulle. La distribution sous l'hypothèse nulle correspond à la distribution théorique de la statistique du test en supposant que l'effet que nous testons n'a pas d'impact (i.e. que nos variables explicatives n'ont pas d'effet sur la variable dépendante). Dans les tests statistiques classiques, cette distribution correspond à différentes lois connues, la plus connue étant la loi Gaussienne, mais ça peut-être la distribution de Student, la distribution $\chi 2$ etc. La particularité des tests de permutations est que l'on va générer cette distribution théorique de façon empirique directement à partir des données. Les tests de permutation sont des tests non-paramétriques (ils ne font pas d'approximation de distribution) et exacts (basés sur les données uniquement).

Selon les règles du théorème centrale limite, répéter un grand nombre de fois une observation d'une variable aléatoire (ici la statistique de test) ménera à une distribution Gaussienne dans laquelle on pourra placer la statistique de test observée (i.e. calculée sur notre échantillon) et ainsi définir la significativité de l'effet de nos variables explicatives. En d'autres termes, on génère toutes les permutations possibles des observations, calculons la statistique de test pour chaque permutation, puis comparons la statistique observée à cette distribution nulle.


### Statistique de Test
La statistique d'un test est une mesure numérique utilisée pour évaluer si les différences observées entre les groupes dans une étude sont statistiquement significatives. Elle permet de quantifier l'écart entre les observations observées et celles qui pourraient être attribuées au hasard. Voici quelques exemples de statistiques de tests connues :
$$
\begin{align*}
F [\text{ANOVA}] &: \quad F = \frac{MS_{\text{intergroupes}}}{MS_{\text{intragroupes}}} \quad \sim \mathcal{F(ddl_{\text{intergroupes}}, ddl_{\text{intragroupes}})} &&\\
\\
t [\text{t de Student}] &: \quad t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \quad \sim t(ddl) &&\\
\\
\chi^2 [\text{Test du } \chi^2] &: \quad \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i} \quad \sim \chi^2(ddl) &&\\
\end{align*}

$$
Avec $MS$ l'erreur moyenne carrée, $ddl$ le degré de liberté, $\bar{X}_i$ la moyenne du groupe $i$, $s^2_i$ la variance du groupe $i$, $n_i$ le nombre de réplicats du groupe $i$, $O_i$ la distance $\chi^2$ observée, $E_i$ la distance $\chi^2$ attendue. 

Ces statistiques suivent des loi de distribution connues, on va donc comparer la statistique observée (calculé sur l'échantillon de données) à la distribution nulle de cette statistique et obtenir ainsi une p-valeur.


Ces staomme La statistique de test pour les tests de permutations dépend du type d'étude et de la relation entre les données (indépendance ou non-indépendance). Pour un test de différence de moyennes entre deux groupes, par exemple, la statistique de test pourrait être définie comme suit :

\[
T = \frac{\bar{X}_1 - \bar{X}_2}{s_{\text{pooled}}} \sqrt{\frac{n_1 n_2}{n_1 + n_2}},
\]

où \(\bar{X}_1\) et \(\bar{X}_2\) sont les moyennes des deux groupes, \(s_{\text{pooled}}\) est l'écart-type combiné des groupes, et \(n_1\) et \(n_2\) sont les tailles des échantillons.

## Mise en œuvre des Tests de Permutations

### Étapes du Test

Les étapes typiques pour mettre en œuvre un test de permutations comprennent :

1. Formuler les hypothèses nulle et alternative.
2. Calculer la statistique de test à partir des données observées.
3. Générer toutes les permutations possibles des données.
4. Calculer la statistique de test pour chaque permutation.
5. Comparer la statistique observée à la distribution nulle des statistiques de test.
6. Conclure sur l'acceptation ou le rejet de l'hypothèse nulle.


## Exemples Pratiques
### Test de Permutations pour la Différence de Moyennes

Considérons un exemple où nous voulons tester si la différence de moyennes entre deux groupes est statistiquement significative. La statistique de test est définie comme dans l'équation ci-dessus.

### Test de Permutations pour la Corrélation

Dans le cas d'une corrélation, la statistique de test pourrait être basée sur la mesure de corrélation de Spearman. Les étapes décrites précedemment devront être suivi également.


