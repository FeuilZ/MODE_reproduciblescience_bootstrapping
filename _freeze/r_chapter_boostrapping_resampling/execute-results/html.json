{
  "hash": "0c96508dd937371a63899fcffd6d4582",
  "result": {
    "markdown": "---\ntitle: \"Chapter : Bootstrapping and Resampling\"\n\nbibliography: references.bib\nexecute: \n  freeze: auto\noutput: \n  html_document:\n   toc: true\n   toc_float: true\n---\n\n\n# General introduction\n\nA dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- iris\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\n## Confidence interval on a sample\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(data$Sepal.Width)\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nshapiro.test(data$Sepal.Width)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  data$Sepal.Width\nW = 0.98492, p-value = 0.1012\n```\n:::\n\n```{.r .cell-code}\nqqnorm(data$Sepal.Width)\nqqline(data$Sepal.Width)\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\nThe sample being normally distributed, we can estimate a confidence interval as below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlow <- mean(data$Sepal.Width)-1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))\nhigh <- mean(data$Sepal.Width)+1.96*sd(data$Sepal.Width)/sqrt(length(data$Sepal.Width))\nprint(c(low,high))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.987580 3.127086\n```\n:::\n:::\n\n\n## Confidence interval on a sub-sample\n\nLet's imagine now that we only have access to a smaller data set. The full iris dataset is the population from which we want to estimate a statistical parameter for instance the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(data$Sepal.Width)\nprint(mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.057333\n```\n:::\n:::\n\n\nHere $\\mu$ is the \"real\" mean of the population that theoretically we do not know.\\\nWe only have access to the following sub-sample of 15 individuals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_sample <- data[sample(1:150,15),]\npar(mfrow=c(2,1))\nhist(data_sample$Sepal.Width)\nqqnorm(data_sample$Sepal.Width)\nqqline(data_sample$Sepal.Width)\n```\n\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThis sample is not normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(data_sample$Sepal.Width)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.146667\n```\n:::\n:::\n\n\nThe mean is 3.15, the problem is that we can not estimate if the mean that we observe is far from the true mean. We don't have any method with classical statistics to estimate a confidence interval. This is where resampling methods can become interesting.\n\n# The Jackknife resampling method\n\nThis method was first proposed in 1949 by Maurice Quenouille ( **Source Quenouille** ). The name \"Jackknife\" comes from the fact that it is often referenced as a \"quick and dirty\" tool of statistics (**Source Abdi)**. It means that it is usable in many situations but it is often not the best tool. The technique allows us to estimate a confidence interval for some statistics when the dataset is too small and/or when it does not follow a known distribution.\n\n## a - The leave-one-out Jackknife\n\nThe principle of the jackknife is to create subsamples of the initial sample by successively removing one of the individuals. This is the leave-one-out technique.\n\nConcretely, for a sample of $n$ individuals, there will be $n$ subsamples of size $(n-1)$. The $i^{th}$ subsample will be composed of individuals from $1$ to $n$ minus the $i^{th}$ individual. A simple representation of all the subsamples is proposed below with the subsamples in lines and the individual numbers in columns.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](r_chapter_boostrapping_resampling_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe next step is to calculate pseudo-values for each of the new subsamples. The formula for pseudo-values depend on the statistic of interest. In our case we want to estimate the mean, the formula will then be:\n\n$$\nv_{i} = n\\overline{X} - (n-1)\\overline{X}_{-i}\n$$\n\nwith the following variables:\n\n|      Variable       | Meaning                                                                             |\n|:-------------------:|---------------------------------------------------|\n|       $v_{i}$       | Pseudo value of the $i^{th}$ subsample                                              |\n|         $n$         | Total number of individuals                                                         |\n|   $\\overline{X}$    | Mean of the initial sample                                                          |\n| $\\overline{X}_{-i}$ | Mean of the $i^{th}$ subsample, corresponding to all individuals except the $i{th}$ |\n\nLet's write a function which create the subsamples and calculate their pseudo values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npseudo_val <- function(data, theta){\n  #entry : data = the vector of data to which we want to apply the Jackknife\n  #entry : theta = function for the statistic of interest\n  #output : a vector of pseudo values for each subsample\n  n <- length(data)\n  mean_init <- theta(data)\n  pv <- rep(NA,n) #to keep in memory each pseudo value\n  for (i in 1:n) {\n    pv[i] <- n*mean_init - (n-1)*theta(data[-i])\n  }\n  return(pv)\n}\n```\n:::\n\n\nTo try the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(pv <- pseudo_val(data =  data_sample$Sepal.Width, \n                  theta = mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 3.7 2.9 2.8 3.0 2.8 3.0 3.0 3.8 3.3 3.2 2.8 3.0 3.6 3.8 2.5\n```\n:::\n:::\n\n\nThe jackknife estimator of the mean will then be calculated as follow:\n\n$$\n\\begin{align}\n\\hat{\\theta} & = \\frac{\\sum_{i=1}^{n}v_{i}}{n} \\\\\n\\hat{\\theta} & = \\overline{v}\n\\end{align}\n$$\n\nIt corresponds to the mean of the pseudo values $v_i$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mean_pv <- mean(pv))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.146667\n```\n:::\n:::\n\n\nThe Jackknife estimator obtained is 3.15. Note that here it is the same as the initial sample mean but it is not always the case. This technique supposes that the Jackknife estimator is normally distributed and its standard error is calculated as follow:\n\n$$\n\\begin{align}\nSE_\\hat{X} & = \\sqrt{\\frac{\\sum_{i=1}^{n}(v_{i}-\\overline{v})}{n(n-1)}} \\\\\nSE_\\hat{X} & = \\sqrt{\\frac{\\sigma_{v}^{2}}{n}}\n\\end{align}\n$$\n\nWith $\\overline{v}$ being the mean of the pseudo values (and the jackknife estimator).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(SE <- sqrt(var(pv)/length(pv)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1050472\n```\n:::\n:::\n\n\nFrom these we can calculate a confidence interval:\n\n$$\n[\\, \\overline{v} - 1.96 \\, SE_\\hat{X} ; \\overline{v} + 1.96 \\, SE_\\hat{X} \\,]\n$$\n\nNote: the 1.96 value comes from the normal distribution table to obtain a 95% confidence interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\" Lower bound : \",\n    as.character(mean_pv - 1.96*SE),\n    \"\\n\",\n    \"Higher bound : \",\n    as.character(mean_pv + 1.96*SE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Lower bound :  2.94077409489412 \n Higher bound :  3.35255923843921\n```\n:::\n:::\n\n\nOn r, functions already exist to automatically execute the Jackknife. For instance, in the package bootstrap, there is a function jackknife. The function take as entry a vector containing the data (x) and a function indicating which statistic needs to be estimated (theta).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"bootstrap\")\nprint(jackknife(x= data_sample$Sepal.Width,\n                theta = function(x) mean(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$jack.se\n[1] 0.1050472\n\n$jack.bias\n[1] 0\n\n$jack.values\n [1] 3.107143 3.164286 3.171429 3.157143 3.171429 3.157143 3.157143 3.100000\n [9] 3.135714 3.142857 3.171429 3.157143 3.114286 3.100000 3.192857\n\n$call\njackknife(x = data_sample$Sepal.Width, theta = function(x) mean(x))\n```\n:::\n:::\n\n\nThe output of the function include the standard error (\\$jack.se) as describe above. It also include the bias (\\$jack.bias) which is the difference between the initial sample statistic and the jackknife estimated statistic. It is important to note that the output \\$jack.values does not correspond to the pseudo values but to the statistic of interest calculated on every subsample. In out example, it correspond to th mean of each subsample.\n\n## b - The grouped Jackknife\n\n## c - \n",
    "supporting": [
      "r_chapter_boostrapping_resampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}